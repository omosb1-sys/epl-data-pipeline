import duckdb
import polars as pl
import torch
import torch.nn as nn
import torch.optim as optim
import shap
import holoviews as hv
from holoviews import opts
import numpy as np
import pandas as pd
from typing import List, Dict
from sklearn.preprocessing import StandardScaler

# [ì¹œì ˆí•œ ê°€ì´ë“œ] ì‹œê°í™” ë„êµ¬ ì„¤ì • (ì°¨íŠ¸ë¥¼ ê·¸ë¦¼ì²˜ëŸ¼ ê·¸ë ¤ì£¼ëŠ” ì¹œêµ¬ì˜ˆìš”)
hv.extension('bokeh')

# --- PyTorch ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ (AIì˜ ë‡Œ ì„¸í¬ êµ¬ì¡°) ---
class MatchPredictorNN(nn.Module):
    def __init__(self, input_dim, hidden_dim=32, output_dim=3):
        super(MatchPredictorNN, self).__init__()
        # ì•„ì£¼ ê°„ë‹¨í•œ 3ì¸µì§œë¦¬ ì‹ ê²½ë§ì´ì—ìš”. ì…ë ¥ -> ìƒê°1 -> ìƒê°2 -> ê²°ê³¼
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(), # 0ë³´ë‹¤ ì‘ì€ ê°’ì€ ë²„ë¦¬ê³  íŠ¹ì§•ì„ ì¡ì•„ë‚´ëŠ” í•„í„°ì˜ˆìš”
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim) # ìµœì¢… ìŠ¹/ë¬´/íŒ¨ 3ê°œ ì¤‘ í•˜ë‚˜ë¥¼ ê³¨ë¼ìš”
        )
        
    def forward(self, x):
        return self.net(x)

class UltimateAnalyticEngine:
    """
    30ë…„ ì°¨ ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´ê°€ ì„¤ê³„í•œ ì´ˆê³ ì„±ëŠ¥ ë¶„ì„ ì—”ì§„
    3ê°œì›” ì°¨ ë¶„ì„ê°€ë‹˜ì„ ìœ„í•´ ì•„ì£¼ ìì„¸í•œ ì£¼ì„ì„ ë‹¬ì•˜ìŠµë‹ˆë‹¤! ğŸš€
    """
    
    def __init__(self, raw_path: str):
        self.raw_path = raw_path
        # CSVë³´ë‹¤ 10ë°° ë¹ ë¥¸ Parquet íŒŒì¼ ê²½ë¡œë¥¼ ë§Œë“¤ì–´ìš”
        self.parquet_path = raw_path.replace('.csv', '.parquet')
        # ë°ì´í„° ì–¸ì–´(SQL)ë¡œ ì†Œí†µí•  DuckDB ì—”ì§„ì„ ì¤€ë¹„í•©ë‹ˆë‹¤
        self.db = duckdb.connect(':memory:')
        self.model = None # ì•„ì§ ê³µë¶€ ì „ì´ë¼ ë¹„ì–´ìˆì–´ìš”
        
    def layer1_ingestion(self):
        """[1ì¸µ: ë°ì´í„° ê°€ì ¸ì˜¤ê¸°] ì›ì‹œ ë°ì´í„°ë¥¼ ê³ ì† ì—”ì§„ìš©ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤."""
        print("ğŸª„ [Phase 1] ë°ì´í„° ì¸í”„ë¼ êµ¬ì¶•: CSV -> Parquet ê³ ì† ë³€í™˜ ì¤‘...")
        # CSVë¥¼ ì½ì–´ì„œ ê°€ì¥ íš¨ìœ¨ì ì¸ Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥í•´ìš”
        pl.read_csv(self.raw_path).write_parquet(self.parquet_path)
        
        # DuckDBë¡œ Parquet íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì˜µë‹ˆë‹¤. (ì´ê²Œ ì œì¼ ë¹¨ë¼ìš”!)
        query = f"SELECT * FROM read_parquet('{self.parquet_path}')"
        self.df_pl = self.db.query(query).pl()
        return self.df_pl

    def layer2_kinetic_processing(self):
        """[2ì¸µ: íŠ¹ì§• ë§Œë“¤ê¸°] ê¸°ê³„ê°€ ê³µë¶€í•  ìˆ˜ ìˆê²Œ ë°ì´í„°ë¥¼ ìš”ë¦¬í•©ë‹ˆë‹¤."""
        print("âš¡ [Phase 2] Polars ë²¡í„° ì—”ì§„: ë¹›ì˜ ì†ë„ë¡œ íŒ€ë³„ ì§€í‘œ ê³„ì‚°...")
        
        # lazy()ëŠ” 'ê³„ì‚° ê³„íš'ë§Œ ì„¸ìš°ê³  ë‚˜ì¤‘ì— í•œêº¼ë²ˆì— ì‹¤í–‰í•´ì„œ ì—„ì²­ ë¹¨ë¼ìš”!
        self.processed_pl = self.df_pl.lazy().with_columns([
            # ìµœê·¼ 5ê²½ê¸° í‰ê·  ë“ì  (íŒ€ë³„ë¡œ ë²½ì„ ì¹˜ê³  ê³„ì‚°í•´ìš”)
            pl.col("goals_for").rolling_mean(window_size=5, min_periods=1).over("team").alias("avg_goals"),
            # ì§€ê¸ˆê¹Œì§€ ìŒ“ì¸ ëˆ„ì  ìŠ¹ì 
            pl.col("points").cum_sum().over("team").alias("cum_points")
        ]).collect() # ì—¬ê¸°ì„œ ì‹¤ì œë¡œ ìš”ë¦¬ê°€ ì‹œì‘ë©ë‹ˆë‹¤
        return self.processed_pl

    def layer3_advanced_inference(self):
        """[3ì¸µ: ì§„ì§œ AI í•™ìŠµ] PyTorch ì‹ ê²½ë§ì´ ì¶•êµ¬ì˜ ë²•ì¹™ì„ ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤."""
        print("ğŸ§  [Phase 3] ë‹¤ì°¨ì› ëª¨ë¸ë§: PyTorch ì‹ ê²½ë§ ì—”ì§„ ê°€ë™ ì¤‘...")
        
        # 1. ë°ì´í„° ì „ì²˜ë¦¬ (ë¹„ì–´ìˆëŠ” ê°’ì€ ê³¼ê°íˆ ë²„ë¦¬ê³  í•„ìš”í•œ ê²ƒë§Œ ì±™ê²¨ìš”)
        df = self.processed_pl.to_pandas().dropna(subset=['avg_goals', 'cum_points', 'result'])
        
        # 2. ê²°ê³¼(Win/Draw/Loss)ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤ (ê¸°ê³„ëŠ” ìˆ«ìë§Œ ì½ì–´ìš”)
        mapping = {'Win': 2, 'Draw': 1, 'Loss': 0}
        df['target'] = df['result'].map(mapping)
        
        X = df[['avg_goals', 'cum_points']].values # ë¬¸ì œì§€
        y = df['target'].values # ë‹µì•ˆì§€
        
        # 3. ë°ì´í„° í‘œì¤€í™” (ëˆ„ì  ìŠ¹ì ì€ í°ë° í‰ê·  ë“ì ì€ ì‘ìœ¼ë©´ ê¸°ê³„ê°€ í—·ê°ˆë ¤í•´ìš”. ë‹¤ë¦¬ë¯¸ë¡œ í´ì¤ë‹ˆë‹¤!)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # 4. PyTorch ì „ìš© 'í…ì„œ' ë°”êµ¬ë‹ˆì— ë‹´ê¸°
        X_tensor = torch.FloatTensor(X_scaled)
        y_tensor = torch.LongTensor(y)
        
        # 5. ëª¨ë¸ê³¼ í•™ìŠµ ë„êµ¬ ì†Œí™˜
        model = MatchPredictorNN(input_dim=2) # ì…ë ¥ì€ í‰ê· ë“ì ê³¼ ëˆ„ì ìŠ¹ì  2ê°œ!
        criterion = nn.CrossEntropyLoss() # ì •ë‹µì„ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ì²´í¬í•˜ëŠ” 'ì±„ì ê´€'
        optimizer = optim.Adam(model.parameters(), lr=0.01) # ì˜¤ë‹µì„ ì¤„ì´ë ¤ ë…¸ë ¥í•˜ëŠ” 'ì„ ìƒë‹˜'
        
        # 6. ì§„ì§œ ê³µë¶€ ì‹œì‘ (20ë²ˆ ë°˜ë³µí•´ì„œ ë¬¸ì œì§€ë¥¼ í’€ì–´ìš”)
        print("ğŸš€ PyTorch ì‹ ê²½ë§ì´ ë°ì´í„°ë¥¼ ë³´ë©° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¤‘ (Training)...")
        for epoch in range(20):
            optimizer.zero_grad() # ì´ì „ ê¸°ì–µ(ì˜¤ì°¨)ì„ ì§€ìš°ê³  ìƒˆë¡­ê²Œ ì‹œì‘
            outputs = model(X_tensor) # ë¬¸ì œë¥¼ í’€ê³ 
            loss = criterion(outputs, y_tensor) # ì±„ì ì„ ë°›ê³ 
            loss.backward() # ì–´ë””ê°€ í‹€ë ¸ëŠ”ì§€ ê±°ê¾¸ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ì„œ
            optimizer.step() # ë¨¸ë¦¿ì†(ê°€ì¤‘ì¹˜)ì„ ê³ ì¹©ë‹ˆë‹¤!
            
            if (epoch+1) % 5 == 0:
                print(f"  â— Epoch [{epoch+1}/20] í•™ìŠµ ì˜¤ì°¨(Loss): {loss.item():.4f}")
        
        self.model = model
        print("âœ… PyTorch ëª¨ë¸ì´ ì¶•êµ¬ì˜ íŒ¨í„´ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤! ì‹œìŠ¤í…œì— ì¥ì°© ì™„ë£Œ.")

    def layer4_xai_and_viz(self):
        """[4ì¸µ: ê·¸ë¦¼ìœ¼ë¡œ ë³´ê¸°] ì–´ë ¤ìš´ ë°ì´í„°ë¥¼ ëˆˆì— ë³´ì´ëŠ” ì°¨íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
        print("ğŸ‘ï¸ [Phase 4] ì‹œê° ë¶„ì„: HoloViews ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ ìƒì„±...")
        
        # ë°ì´í„°ë¥¼ íŒë‹¤ìŠ¤ë¡œ ë°”ê¿”ì„œ ì°¨íŠ¸ë¥¼ ê·¸ë ¤ìš”
        df_pd = self.processed_pl.to_pandas()
        curve = hv.Curve(df_pd, 'game_id', 'avg_goals', label='ìµœê·¼ 5ê²½ê¸° í‰ê·  ë“ì ')
        scatter = hv.Scatter(df_pd, 'game_id', 'goals_for', label='ê° ê²½ê¸° ì‹¤ì œ ë“ì ')
        
        # ë‘ ì°¨íŠ¸ë¥¼ ê²¹ì¹˜ê³ (*) ì˜ˆì˜ê²Œ ê¾¸ë°‰ë‹ˆë‹¤
        viz = (curve * scatter).opts(
            opts.Curve(width=800, height=400, color='blue', line_width=2, tools=['hover']),
            opts.Scatter(size=6, color='red', alpha=0.5)
        )
        return viz

    def layer5_toon_reporting(self, data: List[Dict]):
        """[5ì¸µ: í•µì‹¬ ë³´ê³ ] AI(ì œë¯¸ë‚˜ì´)ê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ê°€ë²¼ìš´ í¬ë§·ìœ¼ë¡œ ìš”ì•½ ë³´ê³ í•©ë‹ˆë‹¤."""
        print("ğŸ“„ [Phase 5] ì‹œë‹ˆì–´ ë¶„ì„ê°€ ìµœì¢… Insight ë¦¬í¬íŠ¸ (TOON í˜•ì‹)...")
        lines = [f"â–¼ ANALYTICAL_INSIGHT_REPORT"]
        for item in data:
            # íŠ¹ìˆ˜ë¬¸ìë¥¼ ë‹¤ ë¹¼ê³  ê¸€ìë§Œ ë‚¨ê²¨ì„œ AIê°€ ì¶”ë¡ í•˜ê¸° ì œì¼ í¸í•œ ìƒíƒœì˜ˆìš”
            fields = [f"{k}: {v}" for k, v in item.items() if v is not None]
            lines.append(f"  â— " + " | ".join(fields))
        print("\n".join(lines))

# --- ë©”ì¸ ì‹¤í–‰ë¶€ (ê³µì¥ ê°€ë™) ---
if __name__ == "__main__":
    # 1. ê³µì¥ì¥ ì†Œí™˜
    engine = UltimateAnalyticEngine('data/processed/team_match_results.csv')
    
    # 2. ì „ ê³µì • ìˆœì°¨ ê°€ë™ (1ì¸µ -> 2ì¸µ -> 3ì¸µ -> 4ì¸µ -> 5ì¸µ)
    engine.layer1_ingestion()
    df_pro = engine.layer2_kinetic_processing()
    engine.layer3_advanced_inference()
    
    # 4ì¸µ ì‹œê°í™” ê²°ê³¼ë¬¼ (ì£¼í”¼í„° ë…¸íŠ¸ë¶ í™˜ê²½ì´ë¼ë©´ ìë™ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤)
    viz = engine.layer4_xai_and_viz()
    
    # 5ì¸µ ìµœì¢… TOON ë¦¬í¬íŠ¸ ì¶œë ¥
    summary = df_pro.head(3).to_dicts()
    engine.layer5_toon_reporting(summary)
    
    print("\nâœ… [ì•ˆí‹°ê·¸ë˜ë¹„í‹°] ì´ˆë³´ ë¶„ì„ê°€ë‹˜, ì´ì œ ëª¨ë“  ì¸ê³µì§€ëŠ¥ ë¶„ì„ì´ ëë‚¬ìŠµë‹ˆë‹¤! í™”ì´íŒ…! ğŸ’ª")
