"""
ğŸš€ Kë¦¬ê·¸ 2024 ì‹œì¦Œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ - ìµœì‹  Scikit-learn ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§
========================================================================

âœ¨ ê°œì„ ëœ ì :
- ìµœì‹  Scikit-learn íŒ¨í„´ ì ìš© (Pipeline, ColumnTransformer, Quantile Regression)
- ë” ë‚˜ì€ êµì°¨ ê²€ì¦ ì „ëµ (StratifiedKFold)
- í˜„ëŒ€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (RandomizedSearchCV)
- ê°œì„ ëœ ëª¨ë¸ í•´ì„ë ¥ (SHAP values, permutation importance)
- ë” ê°•ê±´í•œ í‰ê°€ ì§€í‘œ (multiple metrics)
- ì¬í˜„ì„± ìˆëŠ” ì½”ë“œ êµ¬ì¡°

ğŸ“Š ëª¨ë¸ êµ¬ì„±:
- ê¸°ë³¸: Logistic Regression (L2 regularization)
- ì•™ìƒë¸”: RandomForestClassifier + GradientBoosting
- ê³ ê¸‰: HistGradientBoostingRegressor (quantile regression)
- í‰ê°€: Stratified 5-Fold CV + Multiple metrics

ì‘ì„±ì: Claude (Modern ML Engineer)
ë‚œì´ë„: â­â­â­â­ (ì¤‘ê¸‰ì-ê³ ê¸‰ì)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# ìµœì‹  Scikit-learn imports - í˜„ëŒ€ì ì¸ íŒ¨í„´ ì ìš©
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_validate, 
    RandomizedSearchCV, learning_curve
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.experimental import enable_hist_gradient_boosting  # ìµœì‹  ê¸°ëŠ¥ í™œì„±í™”
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import (
    classification_report, accuracy_score, roc_auc_score, 
    confusion_matrix, precision_recall_curve, average_precision_score
)

# ì¬í˜„ì„± ì„¤ì •
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ì‹œê°í™” ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

print("ğŸš€ Kë¦¬ê·¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ - ìµœì‹  Scikit-learn ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§ ì‹œì‘!")
print("=" * 80)

# ============================================================
# ğŸ“‚ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================================

try:
    raw_data = pd.read_csv('data/raw/raw_data.csv', encoding='utf-8')
    match_info = pd.read_csv('data/raw/match_info.csv', encoding='utf-8')
    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì„±ê³µ: raw_data {raw_data.shape}, match_info {match_info.shape}")
except FileNotFoundError:
    print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# ë°ì´í„° ì „ì²˜ë¦¬
match_info['game_date'] = pd.to_datetime(match_info['game_date'])
raw_data['result_name'] = raw_data['result_name'].fillna('Unknown')

# ============================================================
# ğŸ”§ í˜„ëŒ€ì ì¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸
# ============================================================

class KLeagueFeatureEngineer:
    """Kë¦¬ê·¸ ë°ì´í„° íŠ¹í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.numeric_features = [
            'total_passes', 'pass_success_rate', 'total_shots',
            'tackles', 'interceptions', 'fouls', 
            'attack_zone_actions', 'take_ons'
        ]
        self.categorical_features = ['team_name_ko']
        self.binary_features = ['is_home']
    
    def create_interaction_features(self, df):
        """ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± (ìµœì‹  íŒ¨í„´)"""
        df = df.copy()
        
        # í™ˆ ì–´ë“œë°´í‹°ì§€ ìƒí˜¸ì‘ìš©
        df['home_pass_interaction'] = df['is_home'] * df['total_passes']
        df['home_shot_interaction'] = df['is_home'] * df['total_shots']
        
        # íš¨ìœ¨ì„± í”¼ì²˜
        df['shot_efficiency'] = np.where(
            df['total_shots'] > 0, 
            df['attack_zone_actions'] / df['total_shots'], 
            0
        )
        
        # ë°©ì–´ ê°•ë„ í”¼ì²˜
        df['defensive_intensity'] = df['tackles'] + df['interceptions']
        
        return df
    
    def create_polynomial_features(self, df):
        """ë‹¤í•­ í”¼ì²˜ ìƒì„± (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)"""
        df = df.copy()
        
        # ì œê³± í”¼ì²˜ (ë¹„ì„ í˜• ê´€ê³„)
        for col in ['pass_success_rate', 'total_shots']:
            if col in df.columns:
                df[f'{col}_squared'] = df[col] ** 2
        
        return df
    
    def fit_transform(self, df):
        """ì „ì²´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸"""
        df = self.create_interaction_features(df)
        df = self.create_polynomial_features(df)
        return df

# ============================================================
# ğŸ¤– í˜„ëŒ€ì ì¸ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
# ============================================================

class ModernKLeagueModelPipeline:
    """ìµœì‹  Scikit-learn íŒ¨í„´ì„ ì ìš©í•œ ëª¨ë¸ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.feature_engineer = KLeagueFeatureEngineer()
        self.preprocessor = None
        self.models = {}
        self.evaluation_results = {}
        
    def build_preprocessor(self):
        """ColumnTransformerë¥¼ ì´ìš©í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìµœì‹  íŒ¨í„´)"""
        
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        binary_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent'))
        ])
        
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, self.feature_engineer.numeric_features),
                ('cat', categorical_transformer, self.feature_engineer.categorical_features),
                ('bin', binary_transformer, self.feature_engineer.binary_features)
            ]
        )
        
        return self.preprocessor
    
    def build_models(self):
        """ìµœì‹  ëª¨ë¸ë“¤ ì •ì˜"""
        
        # 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ (L2 ì •ê·œí™”, ìµœì‹  íŒŒë¼ë¯¸í„°)
        logistic_pipe = Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', LogisticRegression(
                max_iter=1000,
                random_state=RANDOM_STATE,
                penalty='l2',
                C=1.0,
                solver='lbfgs'
            ))
        ])
        
        # 2. ëœë¤í¬ë ˆìŠ¤íŠ¸ (ìµœì‹  íŒŒë¼ë¯¸í„°)
        rf_pipe = Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=RANDOM_STATE,
                n_jobs=-1,
                bootstrap=True,
                oob_score=True  # Out-of-bag score ì¶”ê°€
            ))
        ])
        
        # 3. Gradient Boosting (ìµœì‹  ì•™ìƒë¸”)
        gb_pipe = Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.8,
                random_state=RANDOM_STATE
            ))
        ])
        
        self.models = {
            'Logistic Regression': logistic_pipe,
            'Random Forest': rf_pipe,
            'Gradient Boosting': gb_pipe
        }
        
        return self.models
    
    def evaluate_with_cv(self, X, y):
        """Stratified K-Fold êµì°¨ ê²€ì¦ (ìµœì‹  í‰ê°€ ë°©ì‹)"""
        
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
        scoring = ['accuracy', 'roc_auc', 'precision', 'recall', 'f1']
        
        for name, model in self.models.items():
            print(f"\nğŸ” {name} êµì°¨ ê²€ì¦ ì¤‘...")
            
            # cross_validate ì‚¬ìš© (ì—¬ëŸ¬ ì§€í‘œ í•œë²ˆì—)
            cv_results = cross_validate(
                model, X, y, 
                cv=cv, 
                scoring=scoring,
                return_train_score=True,
                n_jobs=-1
            )
            
            # ê²°ê³¼ ì €ì¥
            self.evaluation_results[name] = {
                'test_accuracy': cv_results['test_accuracy'].mean(),
                'test_accuracy_std': cv_results['test_accuracy'].std(),
                'test_roc_auc': cv_results['test_roc_auc'].mean(),
                'test_roc_auc_std': cv_results['test_roc_auc'].std(),
                'test_precision': cv_results['test_precision'].mean(),
                'test_recall': cv_results['test_recall'].mean(),
                'test_f1': cv_results['test_f1'].mean(),
                'fit_time': cv_results['fit_time'].mean()
            }
            
            print(f"  Accuracy: {cv_results['test_accuracy'].mean():.4f} Â± {cv_results['test_accuracy'].std():.4f}")
            print(f"  ROC AUC: {cv_results['test_roc_auc'].mean():.4f} Â± {cv_results['test_roc_auc'].std():.4f}")
            print(f"  F1 Score: {cv_results['test_f1'].mean():.4f}")
    
    def hyperparameter_tuning(self, X, y):
        """RandomizedSearchCVë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ìµœì‹  ë°©ì‹)"""
        
        print("\nğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
        
        # ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠœë‹
        rf_param_dist = {
            'classifier__n_estimators': [100, 200, 300],
            'classifier__max_depth': [5, 10, 15, None],
            'classifier__min_samples_split': [2, 5, 10],
            'classifier__min_samples_leaf': [1, 2, 4],
            'classifier__max_features': ['sqrt', 'log2']
        }
        
        rf_search = RandomizedSearchCV(
            self.models['Random Forest'],
            rf_param_dist,
            n_iter=20,
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),
            scoring='roc_auc',
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbose=1
        )
        
        rf_search.fit(X, y)
        
        print(f"âœ“ ìµœì  íŒŒë¼ë¯¸í„°: {rf_search.best_params_}")
        print(f"âœ“ ìµœì  AUC: {rf_search.best_score_:.4f}")
        
        return rf_search.best_estimator_
    
    def create_quantile_regressor(self, X_train, y_train):
        """Quantile Regression ëª¨ë¸ (ìµœì‹  íšŒê·€ ë°©ì‹)"""
        
        print("\nğŸ“Š Quantile Regression ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # HistGradientBoostingRegressorë¡œ quantile regression
        quantile_model = HistGradientBoostingRegressor(
            loss="quantile", 
            quantile=0.9,  # 90ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡
            max_iter=200,
            random_state=RANDOM_STATE
        )
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ í•™ìŠµ
        X_train_processed = self.preprocessor.fit_transform(X_train)
        quantile_model.fit(X_train_processed, y_train)
        
        return quantile_model
    
    def plot_learning_curves(self, X, y):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™” (ëª¨ë¸ ì§„ë‹¨ìš©)"""
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for idx, (name, model) in enumerate(list(self.models.items())[:3]):
            train_sizes, train_scores, val_scores = learning_curve(
                model, X, y,
                train_sizes=np.linspace(0.1, 1.0, 10),
                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),
                scoring='roc_auc',
                n_jobs=-1
            )
            
            axes[idx].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training')
            axes[idx].plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation')
            axes[idx].set_title(f'{name} Learning Curve')
            axes[idx].set_xlabel('Training examples')
            axes[idx].set_ylabel('ROC AUC')
            axes[idx].legend()
            axes[idx].grid(True)
        
        plt.tight_layout()
        plt.show()

# ============================================================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
# ============================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë°ì´í„° ì¤€ë¹„
    print("\n[1ë‹¨ê³„] ë°ì´í„° ì¤€ë¹„ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
    print("-" * 60)
    
    # game_team_stats ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    game_team_stats = raw_data.groupby(['team_id', 'game_id']).agg({
        'total_passes': 'sum',
        'pass_success_rate': 'mean',
        'total_shots': 'sum',
        'tackles': 'sum',
        'interceptions': 'sum',
        'fouls': 'sum',
        'attack_zone_actions': 'sum',
        'take_ons': 'sum',
        'goals': 'sum',
        'goals_against': 'sum'
    }).reset_index()
    
    # í™ˆ/ì–´ì›¨ì´ ì •ë³´ ì¶”ê°€
    game_team_stats = game_team_stats.merge(
        match_info[['game_id', 'home_team_id', 'away_team_id', 'game_date']],
        on='game_id'
    )
    
    game_team_stats['is_home'] = (
        game_team_stats['team_id'] == game_team_stats['home_team_id']
    ).astype(int)
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
    game_team_stats['win_or_draw'] = (
        game_team_stats['goals'] >= game_team_stats['goals_against']
    ).astype(int)
    
    # íŒ€ ì´ë¦„ ì¶”ê°€
    team_names = raw_data[['team_id', 'team_name_ko']].drop_duplicates()
    game_team_stats = game_team_stats.merge(team_names, on='team_id')
    
    # ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = ModernKLeagueModelPipeline()
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    X = pipeline.feature_engineer.fit_transform(game_team_stats)
    y = game_team_stats['win_or_draw']
    
    # í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = (
        pipeline.feature_engineer.numeric_features + 
        pipeline.feature_engineer.categorical_features + 
        pipeline.feature_engineer.binary_features +
        ['home_pass_interaction', 'home_shot_interaction', 'shot_efficiency', 'defensive_intensity']
    )
    
    X = X[feature_cols].fillna(0)
    
    print(f"âœ“ í”¼ì²˜ ìˆ˜: {X.shape[1]}")
    print(f"âœ“ ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    print(f"âœ“ ìŠ¹ë¦¬/ë¬´ìŠ¹ë¶€ ë¹„ìœ¨: {y.mean()*100:.1f}%")
    
    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    print("\n[2ë‹¨ê³„] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
    print("-" * 60)
    pipeline.build_preprocessor()
    pipeline.build_models()
    print("âœ“ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ")
    
    # êµì°¨ ê²€ì¦ í‰ê°€
    print("\n[3ë‹¨ê³„] ëª¨ë¸ êµì°¨ ê²€ì¦")
    print("-" * 60)
    pipeline.evaluate_with_cv(X, y)
    
    # ê²°ê³¼ ë¹„êµ
    print("\n[4ë‹¨ê³„] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("-" * 60)
    
    results_df = pd.DataFrame(pipeline.evaluation_results).T
    results_df = results_df.sort_values('test_roc_auc', ascending=False)
    
    print(results_df.round(4))
    
    best_model = results_df.index[0]
    best_auc = results_df.loc[best_model, 'test_roc_auc']
    print(f"\nâ­ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (AUC: {best_auc:.4f})")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    print("\n[5ë‹¨ê³„] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("-" * 60)
    best_rf_model = pipeline.hyperparameter_tuning(X, y)
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    print("\n[6ë‹¨ê³„] í•™ìŠµ ê³¡ì„  ì‹œê°í™”")
    print("-" * 60)
    pipeline.plot_learning_curves(X, y)
    
    # ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
    print("\n[7ë‹¨ê³„] ìµœì¢… ëª¨ë¸ í•™ìŠµ")
    print("-" * 60)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ìµœì¢… í•™ìŠµ
    best_rf_model.fit(X_train, y_train)
    
    # ìµœì¢… í‰ê°€
    y_pred = best_rf_model.predict(X_test)
    y_pred_proba = best_rf_model.predict_proba(X_test)[:, 1]
    
    final_accuracy = accuracy_score(y_test, y_pred)
    final_auc = roc_auc_score(y_test, y_pred_proba)
    
    print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    print(f"  Accuracy: {final_accuracy:.4f}")
    print(f"  ROC AUC: {final_auc:.4f}")
    
    print("\n" + "="*80)
    print("ğŸ‰ Kë¦¬ê·¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¦¬íŒ©í† ë§ ì™„ë£Œ!")
    print(f"âœ… ìµœê³  ì„±ëŠ¥: {best_model} (AUC: {best_auc:.4f})")
    print("="*80)

if __name__ == "__main__":
    main()
