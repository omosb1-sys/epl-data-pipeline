"""
[Kmong Project] Production Pipeline v6.0 (Quantum Leap - ML Readiness)
=====================================================================
- Rule 8.1: Advanced Feature Discretization & Log Transformation
- Rule 17: Multi-stage Quality Scaffolding (DQS)
- Rule 33: Russ Cox Protocol (High-Precision Scaling)
- Rule 38: Types as Proofs (Analytical Integrity)
- ML/Statistics: Outlier Management (IQR), Skewness Correction, Robust Scaling

Author: Antigravity AI (Senior Analyst)
Date: 2026-01-26
"""

import polars as pl
import os
import yaml
import gc
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from loguru import logger

def setup_logging():
    """ê³ í•´ìƒë„ ë¡œê¹… ì„¤ì •"""
    log_dir = Path("logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    logger.remove()
    logger.add(
        sink=lambda msg: print(msg, end=""), 
        colorize=True, 
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>"
    )
    logger.add(
        "logs/kmong_production_v6.log", 
        rotation="10 MB", 
        retention="10 days", 
        compression="zip", 
        level="INFO"
    )

class KmongQuantumPipeline:
    """ê³ ê¸‰ í†µê³„ ì§€ëŠ¥ì´ íƒ‘ì¬ëœ 6ì„¸ëŒ€ ë°ì´í„° ìì‚°í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path: str = "config/kmong_settings_v6.yaml"):
        self.project_root = Path(__file__).resolve().parent
        self.config_path = self.project_root / config_path
        self.load_config()
        self.start_time = datetime.now()
        self.metadata = {"pipeline_version": "6.0", "stats": {}}
        
    def load_config(self) -> None:
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.success(f"âœ… V6 Analytics Config Loaded: {self.config_path.name}")
        except Exception as e:
            logger.error(f"âš ï¸ Config error: {e}. Falling back to default.")
            self.config = {'paths': {'raw_dir': 'data/kmong_project/raw', 'processed_dir': 'data/kmong_project/processed_v6'}}

    def run(self) -> None:
        logger.info(f"ğŸ—ï¸ [Quantum Leap] Starting Advanced ML-Ready Pipeline")
        try:
            # 1. ìˆ˜ì§‘ ë° ê¸°ì´ˆ ê°€ê³µ (Streaming)
            temp_parquets = self.ingest_and_clean()
            if not temp_parquets: return

            # 2. ë³´ì•ˆ ë ˆì´ì–´ (Rule 39)
            lf = self.apply_security(temp_parquets)

            # 3. ê³ ê¸‰ í†µê³„ ë¶„ì„ ë ˆì´ì–´ (Statistical Intelligence)
            lf, stats_report = self.apply_statistical_intelligence(lf)
            self.metadata["stats"] = stats_report

            # 4. ML íŠ¹ì§• ê³µí•™ ë ˆì´ì–´ (Feature Engineering)
            lf = self.apply_ml_features(lf)

            # 5. ìµœì¢… ë°ì´í„° ìì‚°í™” (Assetization)
            self.finalize_asset(lf)

            # 6. í’ˆì§ˆ í’ˆì§ˆ ê²Œì´íŠ¸ (Data Quality Score)
            self._execute_quality_gate()

            logger.success(f"ğŸ† Quantum Leap complete. Analytics metadata saved.")
        except Exception as e:
            logger.critical(f"ğŸ”¥ Pipeline crash: {e}")
            raise

    def ingest_and_clean(self) -> List[Path]:
        raw_dir = self.project_root / self.config['paths']['raw_dir']
        processed_dir = self.project_root / self.config['paths']['processed_dir']
        processed_dir.mkdir(parents=True, exist_ok=True)
        
        raw_files = list(raw_dir.glob("**/*.xlsm")) + list(raw_dir.glob("**/*.xlsx"))
        temp_paths = []
        
        for i, f in enumerate(raw_files):
            logger.info(f"ğŸ“¦ [Ingest] {f.name}")
            # [Senior Tip] ë°ì´í„° ì½ê¸° ì „ ë©”ëª¨ë¦¬ í”ŒëŸ¬ì‹±
            gc.collect()
            
            try:
                import fastexcel
                excel = fastexcel.read_excel(f)
                sheets = [pl.from_pandas(excel.load_sheet(sn).to_pandas()).cast(pl.String) for sn in excel.sheet_names]
                df = pl.concat(sheets, how="diagonal")
                
                # ê¸°ì´ˆ ìŠ¤í‚¤ë§ˆ ê°•ì œ (ë¬¸ìì—´ ì •ê·œí™”)
                df = df.with_columns(pl.all().str.strip_chars())
                
                out_path = processed_dir / f"v6_tmp_{f.stem}_{i}.parquet"
                df.write_parquet(out_path)
                temp_paths.append(out_path)
            except Exception as e:
                logger.error(f"âŒ Ingest fail: {f.name} -> {e}")
                
        return temp_paths

    def apply_security(self, parquet_files: List[Path]) -> pl.LazyFrame:
        lf = pl.concat([pl.scan_parquet(f) for f in parquet_files], how="diagonal")
        pii_cols = self.config['security'].get('pii_columns', [])
        
        # Rule 39: ìµëª…í™” ë° ì¶”ì  ì‹ë³„ì ì£¼ì…
        for col in lf.columns:
            if any(k in col.lower() for k in pii_cols) or any(k in col.lower() for k in ['ì„±ëª…', 'ì£¼ì†Œ', 'ì „í™”']):
                lf = lf.with_columns((pl.col(col).str.slice(0, 2) + pl.lit("***")).alias(col))
        return lf

    def apply_statistical_intelligence(self, lf: pl.LazyFrame) -> Tuple[pl.LazyFrame, Dict]:
        """[ê³ ê¸‰ í†µê³„] Outlier íƒì§€ ë° Skewness ë¶„ì„"""
        logger.info("ğŸ§ª [Intelligence] Statistical profiling starting...")
        
        # ë¶„ì„ ëŒ€ìƒ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¶”ì¶œ
        target_cols = self.config['analysis'].get('ml_ready_columns', [])
        stats_report = {}

        for col in target_cols:
            if col in lf.columns:
                # 1. ë°ì´í„° í´ë Œì§• (ìˆ«ìí™”) ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Mean ì „ëµ ì ìš©)
                lf = lf.with_columns(
                    pl.col(col).str.replace_all("[^0-9.]", "").cast(pl.Float64, strict=False).fill_null(strategy="mean")
                )
                
                # 2. í†µê³„ ì§€í‘œ ê³„ì‚° (Lazy ìƒíƒœì—ì„œëŠ” ì§ì ‘ ê³„ì‚°ì´ ì–´ë ¤ìš°ë¯€ë¡œ ìƒ˜í”Œ ì¶”ì¶œ í›„ ê³„ì‚°)
                # [Senior Logic] ëŒ€ê·œëª¨ ë°ì´í„°ì˜ ê²½ìš° ìƒ˜í”Œë§ ê¸°ë°˜ í†µê³„ ì¶”ì •
                sample_df = lf.select(col).limit(10000).collect()
                series = sample_df[col]
                
                # ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë‘ Nullì¸ ê²½ìš° ìŠ¤í‚µ
                if series.null_count() == len(series) or len(series) == 0:
                    logger.warning(f"  âš ï¸ Skipping stats for '{col}': No valid data.")
                    continue

                q1 = series.quantile(0.25)
                q3 = series.quantile(0.75)
                
                # NoneType ë°©ì–´
                if q1 is None or q3 is None:
                    logger.warning(f"  âš ï¸ Quantile calculation returned None for '{col}'.")
                    continue

                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                # Skewness (ì™œê³¡ë„)
                skew = series.skew()
                if skew is None: skew = 0.0
                
                stats_report[col] = {
                    "q1": float(q1), "q3": float(q3), "iqr": float(iqr),
                    "lower_bound": float(lower_bound), "upper_bound": float(upper_bound),
                    "skewness": float(skew)
                }
                
                # 3. ì´ìƒì¹˜ í”Œë˜ê·¸ ì£¼ì… (ML ëª¨ë¸ì—ê²Œ ì´ìƒì¹˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ì•Œë¦¼)
                lf = lf.with_columns(
                    pl.when((pl.col(col) < lower_bound) | (pl.col(col) > upper_bound))
                    .then(pl.lit(1)).otherwise(pl.lit(0)).alias(f"{col}_is_outlier")
                )

        return lf, stats_report

    def apply_ml_features(self, lf: pl.LazyFrame) -> pl.LazyFrame:
        """[ML ì •í™•ë„ í–¥ìƒ] ë¡œê·¸ ë³€í™˜ ë° Robust Scaling ì•„í‚¤í…ì²˜"""
        logger.info("ğŸ—ï¸ [Features] ML-Ready transformation (Log & Robust) path...")
        
        target_cols = self.config['analysis'].get('ml_ready_columns', [])
        skew_threshold = self.config['analysis'].get('skewness_threshold', 0.75)

        for col in target_cols:
            if col in self.metadata["stats"]:
                col_stats = self.metadata["stats"][col]
                
                # 1. ìë™ ë¡œê·¸ ë³€í™˜ (Skewness ê·¹ë³µ)
                if abs(col_stats["skewness"]) > skew_threshold:
                    logger.info(f"   ğŸ“ˆ Log-Transform applied to '{col}' (Skew: {col_stats['skewness']:.2f})")
                    lf = lf.with_columns(
                        (pl.col(col) + 1).log().alias(f"{col}_log")
                    )
                
                # 2. Robust Scaling (IQR ê¸°ë°˜)
                # ê³µì‹: (x - Q2) / (Q3 - Q1)
                median_val = col_stats["q1"] + (col_stats["iqr"] / 2) # ê·¼ì‚¬ê°’
                if col_stats["iqr"] > 0:
                    lf = lf.with_columns(
                        ((pl.col(col) - median_val) / col_stats["iqr"]).alias(f"{col}_robust_scaled")
                    )
        
        # 3. ì‹œê°„ í”¼ì²˜ (Seasonality)
        if "ë“±ë¡ì¼ì" in lf.columns:
            lf = lf.with_columns(
                pl.col("ë“±ë¡ì¼ì").str.strptime(pl.Date, format="%Y-%m-%d", strict=False)
            ).with_columns([
                pl.col("ë“±ë¡ì¼ì").dt.month().alias("reg_month"),
                pl.col("ë“±ë¡ì¼ì").dt.weekday().alias("reg_dow")
            ])

        return lf

    def finalize_asset(self, lf: pl.LazyFrame) -> None:
        """ìµœì¢… ë°ì´í„° ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        master_path = self.project_root / self.config['paths']['master_parquet']
        metadata_path = self.project_root / self.config['paths']['metadata_json']
        
        logger.info(f"ğŸ“¡ [Assetization] Writing master file: {master_path.name}")
        df = lf.collect(streaming=True)
        df.write_parquet(master_path, compression="zstd")
        
        with open(metadata_path, "w", encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=4, ensure_ascii=False)

    def _execute_quality_gate(self) -> None:
        """[Rule 17] Data Quality Score (DQS) í‰ê°€"""
        master_path = self.project_root / self.config['paths']['master_parquet']
        df = pl.read_parquet(master_path, n_rows=1000)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
        scores = {
            "Completeness": (1 - df.null_count().sum().to_numpy().sum() / (df.width * df.height)) * 100,
            "ML_Ready": (len([c for c in df.columns if "log" in c or "scaled" in c]) / len(self.config['analysis']['ml_ready_columns'])) * 100,
            "Security": 100 if "ì°¨ëŒ€ë²ˆí˜¸" in df.columns and "***" in str(df["ì°¨ëŒ€ë²ˆí˜¸"][0]) else 0
        }
        
        avg_score = sum(scores.values()) / len(scores)
        self.metadata["quality_score"] = avg_score
        
        logger.info(f"ğŸ“Š [DQS Report] Total Quality Score: {avg_score:.1f}/100")
        for k, v in scores.items():
            logger.info(f"   - {k}: {v:.1f}")

if __name__ == "__main__":
    setup_logging()
    pipeline = KmongQuantumPipeline()
    pipeline.run()
