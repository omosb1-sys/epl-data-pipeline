import http.client
import json
import os
import time
from datetime import datetime
import requests # [NEW] News scraping
from bs4 import BeautifulSoup # [NEW] News scraping

# ==========================================
# ğŸ”§ ì„¤ì • (Configuration)
# ==========================================
# ==========================================
# ğŸ”§ ì„¤ì • (Configuration)
# ==========================================
# [UPDATE] RapidAPI Deprecation -> Official Direct API
API_HOST = "v3.football.api-sports.io"
# API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ (GitHub Secrets ì‚¬ìš© ê¶Œì¥)
API_KEY = os.getenv("RAPIDAPI_KEY", "") 

# ì €ì¥ ê²½ë¡œ
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
OUTPUT_FILE = os.path.join(DATA_DIR, "latest_epl_data.json")

# EPL League ID (39 = Premier League)
LEAGUE_ID = 39
SEASON = 2024 # 2024-2025 Season

def fetch_from_api(endpoint):
    """
    API-Football Direct (v3)ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    if not API_KEY:
        print("âŒ Error: API Key is missing.")
        return None

    conn = http.client.HTTPSConnection(API_HOST)
    headers = {
        'x-apisports-key': API_KEY
    }
    
    try:
        print(f"ğŸ“¡ Requesting: {endpoint}...")
        conn.request("GET", endpoint, headers=headers)
        res = conn.getresponse()
        data = res.read()
        
        if res.status != 200:
            print(f"âŒ API Error {res.status}: {res.reason}")
            return None
            
        return json.loads(data.decode("utf-8"))
    except Exception as e:
        print(f"âŒ Connection Error: {e}")
        return None
    finally:
        conn.close()

def scrape_epl_news():
    """Google News RSSë¥¼ í†µí•´ EPL ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ë§¤ìš° ì•ˆì •ì )"""
    print("ğŸ“¡ Fetching news from Google News RSS...")
    # ì˜ë¬¸ ë‰´ìŠ¤ (Premier League ê²€ìƒ‰)
    url = "https://news.google.com/rss/search?q=Premier+League+News&hl=en-GB&gl=GB&ceid=GB:en"
    news_list = []
    
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'xml') # XML íŒŒì‹±
        
        items = soup.find_all('item', limit=15)
        for item in items:
            title = item.title.text
            link = item.link.text
            source = item.source.text if item.source else "Google News"
            
            news_list.append({
                "source": source,
                "title": title,
                "url": link
            })
            
        # í•œê¸€ ë‰´ìŠ¤ ì¶”ê°€ (í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ ê²€ìƒ‰)
        url_ko = "https://news.google.com/rss/search?q=í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸&hl=ko&gl=KR&ceid=KR:ko"
        res_ko = requests.get(url_ko, timeout=10)
        soup_ko = BeautifulSoup(res_ko.text, 'xml')
        items_ko = soup_ko.find_all('item', limit=10)
        for item in items_ko:
            news_list.append({
                "source": item.source.text if item.source else "êµ¬ê¸€ ë‰´ìŠ¤",
                "title": item.title.text,
                "url": item.link.text
            })

        print(f"âœ… Total News collected: {len(news_list)} items")
    except Exception as e:
        print(f"âš ï¸ News fetching failed: {e}")
        
    return news_list

def main():
    print("ğŸš€ [EPL Data Robot] Starting data collection...")
    
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        
    final_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "season": SEASON,
        "standings": [],
        "fixtures": [],
        "top_scorers": [],
        "news": [] # [NEW]
    }
    
    # ... API í˜¸ì¶œë¶€ (ìƒëµ) ...
    # 1. Standings
    # 2. Fixtures
    # (ìœ„ì˜ ì½”ë“œê°€ ê³„ì† ìˆë‹¤ê³  ê°€ì •)
    
    # [FIX] main í•¨ìˆ˜ì˜ íë¦„ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì½”ë“œë¥¼ ì •í™•íˆ ë§¤ì¹­í•´ì„œ ë„£ì–´ì¤ë‹ˆë‹¤.
    # 1. Standings
    standings_data = fetch_from_api(f"/v3/standings?season={SEASON}&league={LEAGUE_ID}")
    if standings_data and standings_data.get('response'):
        final_data['standings'] = standings_data['response'][0]['league']['standings'][0]
        print(f"âœ… Standings collected.")

    # 2. Fixtures
    current_round_resp = fetch_from_api(f"/v3/fixtures/rounds?season={SEASON}&league={LEAGUE_ID}&current=true")
    if current_round_resp and current_round_resp.get('response'):
        current_round = current_round_resp['response'][0]
        fixtures_data = fetch_from_api(f"/v3/fixtures?season={SEASON}&league={LEAGUE_ID}&round={current_round}")
        if fixtures_data and fixtures_data.get('response'):
            final_data['fixtures'] = fixtures_data['response']
            print(f"âœ… Fixtures collected.")

    # 4. News Scraping
    final_data['news'] = scrape_epl_news()

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=4, ensure_ascii=False)
        
    print(f"ğŸ’¾ Data saved to: {OUTPUT_FILE}")
    print("âœ¨ Mission Complete!")

if __name__ == "__main__":
    main()
