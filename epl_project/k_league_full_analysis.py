"""
K-ë¦¬ê·¸ í’€ìŠ¤íƒ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸
=====================================
[EDA â†’ íŒŒìƒë³€ìˆ˜ â†’ í†µê³„ë¶„ì„ â†’ ë¨¸ì‹ ëŸ¬ë‹ â†’ ì¸ì‚¬ì´íŠ¸]

Author: Antigravity (Senior Data Analyst)
Date: 2026-01-21
"""

import polars as pl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import warnings
import os
from typing import Tuple, Dict, Any

warnings.filterwarnings('ignore')

# ============================================
# 0. í™˜ê²½ ì„¤ì •
# ============================================
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-whitegrid')

BASE_PATH = "/Users/sebokoh/ë°ì´í„°ë¶„ì„ì—°ìŠµ/ë°ì´ì½˜/kë¦¬ê·¸ë°ì´í„°/ë¦¬ê·¸ë°ì´í„°/epl_project/data"
OUTPUT_PATH = "/Users/sebokoh/ë°ì´í„°ë¶„ì„ì—°ìŠµ/ë°ì´ì½˜/kë¦¬ê·¸ë°ì´í„°/ë¦¬ê·¸ë°ì´í„°/epl_project/output"
os.makedirs(OUTPUT_PATH, exist_ok=True)


def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """ë°ì´í„° ë¡œë“œ ë° Pandas ë³€í™˜"""
    print("=" * 60)
    print("ğŸ“Š [STEP 0] ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    
    match_info = pd.read_csv(f"{BASE_PATH}/match_info.csv")
    raw_data = pd.read_csv(f"{BASE_PATH}/raw_data.csv")
    
    print(f"âœ… match_info: {match_info.shape}")
    print(f"âœ… raw_data: {raw_data.shape}")
    
    return match_info, raw_data


# ============================================
# 1. íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering)
# ============================================
def create_features(match_df: pd.DataFrame, event_df: pd.DataFrame) -> pd.DataFrame:
    """ê²½ê¸°ë³„ ì§‘ê³„ íŒŒìƒë³€ìˆ˜ ìƒì„±
    
    Returns:
        ê²½ê¸° ë‹¨ìœ„ë¡œ ì§‘ê³„ëœ ë¶„ì„ìš© ë°ì´í„°í”„ë ˆì„
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ [STEP 1] íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering)")
    print("=" * 60)
    
    # 1-1. ê²½ê¸°ë³„ ì´ë²¤íŠ¸ ì§‘ê³„
    game_stats = event_df.groupby(['game_id', 'team_id']).agg(
        total_actions=('action_id', 'count'),
        total_passes=('type_name', lambda x: (x == 'Pass').sum()),
        total_shots=('type_name', lambda x: (x == 'Shot').sum()),
        successful_actions=('result_name', lambda x: (x == 'Successful').sum()),
        avg_x_position=('start_x', 'mean'),
        avg_y_position=('start_y', 'mean'),
        max_x_reach=('end_x', 'max'),
        unique_players=('player_id', 'nunique')
    ).reset_index()
    
    # 1-2. íŒŒìƒë¹„ìœ¨ ê³„ì‚°
    game_stats['pass_ratio'] = game_stats['total_passes'] / game_stats['total_actions']
    game_stats['shot_ratio'] = game_stats['total_shots'] / game_stats['total_actions']
    game_stats['success_rate'] = game_stats['successful_actions'] / game_stats['total_actions']
    
    # 1-3. ê²½ê¸° ë©”íƒ€ë°ì´í„° ë³‘í•©
    merged = game_stats.merge(
        match_df[['game_id', 'home_team_id', 'away_team_id', 'home_score', 'away_score', 
                  'home_team_name_ko', 'away_team_name_ko', 'game_date']],
        on='game_id',
        how='left'
    )
    
    # 1-4. ìŠ¹/ë¬´/íŒ¨ ë¼ë²¨ ìƒì„±
    def get_result(row):
        if row['team_id'] == row['home_team_id']:
            if row['home_score'] > row['away_score']:
                return 'Win'
            elif row['home_score'] < row['away_score']:
                return 'Lose'
            else:
                return 'Draw'
        else:
            if row['away_score'] > row['home_score']:
                return 'Win'
            elif row['away_score'] < row['home_score']:
                return 'Lose'
            else:
                return 'Draw'
    
    merged['result'] = merged.apply(get_result, axis=1)
    
    print(f"âœ… ìƒì„±ëœ íŒŒìƒë³€ìˆ˜: {list(game_stats.columns[2:])}")
    print(f"âœ… ìµœì¢… ë°ì´í„° Shape: {merged.shape}")
    
    return merged


# ============================================
# 2. ê¸°ë³¸ í†µê³„ ë¶„ì„
# ============================================
def basic_statistics(df: pd.DataFrame) -> Dict[str, Any]:
    """ê¸°ìˆ í†µê³„ëŸ‰ ë° ë¶„í¬ ë¶„ì„"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ [STEP 2] ê¸°ë³¸ í†µê³„ ë¶„ì„")
    print("=" * 60)
    
    numeric_cols = ['total_actions', 'total_passes', 'total_shots', 
                    'success_rate', 'pass_ratio', 'shot_ratio']
    
    # 2-1. ê¸°ìˆ í†µê³„ëŸ‰
    desc_stats = df[numeric_cols].describe()
    print("\n[ê¸°ìˆ í†µê³„ëŸ‰]")
    print(desc_stats.round(3))
    
    # 2-2. ê²°ê³¼ë³„ í†µê³„
    result_stats = df.groupby('result')[numeric_cols].mean()
    print("\n[ìŠ¹/ë¬´/íŒ¨ë³„ í‰ê·  ì§€í‘œ]")
    print(result_stats.round(3))
    
    # 2-3. ë¶„í¬ ì‹œê°í™”
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, col in enumerate(numeric_cols):
        sns.histplot(df[col], kde=True, ax=axes[idx], color='steelblue')
        axes[idx].set_title(f'{col} ë¶„í¬', fontsize=12)
        axes[idx].axvline(df[col].mean(), color='red', linestyle='--', label='í‰ê· ')
        axes[idx].legend()
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/basic_stats_distribution.png", dpi=150)
    print(f"\nğŸ¨ ì €ì¥: {OUTPUT_PATH}/basic_stats_distribution.png")
    
    return {'descriptive': desc_stats, 'by_result': result_stats}


# ============================================
# 3. ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================
def correlation_analysis(df: pd.DataFrame) -> pd.DataFrame:
    """ìƒê´€ê´€ê³„ ë¶„ì„ ë° íˆíŠ¸ë§µ"""
    print("\n" + "=" * 60)
    print("ğŸ”— [STEP 3] ìƒê´€ê´€ê³„ ë¶„ì„")
    print("=" * 60)
    
    numeric_cols = ['total_actions', 'total_passes', 'total_shots', 
                    'success_rate', 'pass_ratio', 'avg_x_position', 'unique_players']
    
    corr_matrix = df[numeric_cols].corr()
    
    # íˆíŠ¸ë§µ
    plt.figure(figsize=(10, 8))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', 
                center=0, fmt='.2f', square=True, linewidths=0.5)
    plt.title('K-ë¦¬ê·¸ ê²½ê¸° ì§€í‘œ ìƒê´€ê´€ê³„ í–‰ë ¬', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/correlation_heatmap.png", dpi=150)
    print(f"ğŸ¨ ì €ì¥: {OUTPUT_PATH}/correlation_heatmap.png")
    
    # ì£¼ìš” ìƒê´€ê´€ê³„ ì¶œë ¥
    print("\n[ì£¼ìš” ìƒê´€ê´€ê³„ (|r| > 0.5)]")
    for i in range(len(numeric_cols)):
        for j in range(i+1, len(numeric_cols)):
            if abs(corr_matrix.iloc[i, j]) > 0.5:
                print(f"  â€¢ {numeric_cols[i]} â†” {numeric_cols[j]}: {corr_matrix.iloc[i, j]:.3f}")
    
    return corr_matrix


# ============================================
# 4. ê³ ê¸‰ í†µê³„ ë¶„ì„
# ============================================
def advanced_statistics(df: pd.DataFrame) -> Dict[str, Any]:
    """ê°€ì„¤ê²€ì • ë° ê³ ê¸‰ í†µê³„ë¶„ì„"""
    print("\n" + "=" * 60)
    print("ğŸ§ª [STEP 4] ê³ ê¸‰ í†µê³„ ë¶„ì„")
    print("=" * 60)
    
    results = {}
    
    # 4-1. ì •ê·œì„± ê²€ì • (Shapiro-Wilk)
    print("\n[ì •ê·œì„± ê²€ì • (Shapiro-Wilk)]")
    for col in ['success_rate', 'pass_ratio']:
        stat, p = stats.shapiro(df[col].dropna().sample(min(500, len(df))))
        normality = "ì •ê·œë¶„í¬ O" if p > 0.05 else "ì •ê·œë¶„í¬ X"
        print(f"  â€¢ {col}: W={stat:.4f}, p={p:.4f} â†’ {normality}")
        results[f'{col}_normality'] = {'statistic': stat, 'p_value': p}
    
    # 4-2. ANOVA (ìŠ¹/ë¬´/íŒ¨ ê·¸ë£¹ ê°„ ì°¨ì´)
    print("\n[ANOVA ê²€ì •: ìŠ¹/ë¬´/íŒ¨ ê·¸ë£¹ ê°„ ì„±ê³µë¥  ì°¨ì´]")
    win_data = df[df['result'] == 'Win']['success_rate']
    draw_data = df[df['result'] == 'Draw']['success_rate']
    lose_data = df[df['result'] == 'Lose']['success_rate']
    
    f_stat, p_anova = stats.f_oneway(win_data, draw_data, lose_data)
    significance = "ìœ ì˜ë¯¸í•œ ì°¨ì´ ì¡´ì¬" if p_anova < 0.05 else "ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ"
    print(f"  â€¢ F-í†µê³„ëŸ‰: {f_stat:.4f}, p-value: {p_anova:.4f} â†’ {significance}")
    results['anova'] = {'f_statistic': f_stat, 'p_value': p_anova}
    
    # 4-3. íš¨ê³¼ í¬ê¸° (Cohen's d: Win vs Lose)
    print("\n[íš¨ê³¼ í¬ê¸° (Cohen's d): ìŠ¹ë¦¬ vs íŒ¨ë°°]")
    cohens_d = (win_data.mean() - lose_data.mean()) / np.sqrt(
        ((win_data.std()**2) + (lose_data.std()**2)) / 2
    )
    effect_size = "Large" if abs(cohens_d) > 0.8 else ("Medium" if abs(cohens_d) > 0.5 else "Small")
    print(f"  â€¢ Cohen's d: {cohens_d:.4f} â†’ {effect_size} Effect")
    results['cohens_d'] = cohens_d
    
    # 4-4. ì‹œê°í™”: ê·¸ë£¹ë³„ ë°•ìŠ¤í”Œë¡¯
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    sns.boxplot(x='result', y='success_rate', data=df, order=['Win', 'Draw', 'Lose'], 
                palette='Set2', ax=axes[0])
    axes[0].set_title('ìŠ¹/ë¬´/íŒ¨ë³„ ì„±ê³µë¥  ë¶„í¬', fontsize=12)
    
    sns.boxplot(x='result', y='total_shots', data=df, order=['Win', 'Draw', 'Lose'],
                palette='Set2', ax=axes[1])
    axes[1].set_title('ìŠ¹/ë¬´/íŒ¨ë³„ ìŠˆíŒ… íšŸìˆ˜ ë¶„í¬', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/advanced_stats_boxplot.png", dpi=150)
    print(f"\nğŸ¨ ì €ì¥: {OUTPUT_PATH}/advanced_stats_boxplot.png")
    
    return results


# ============================================
# 5. ë¨¸ì‹ ëŸ¬ë‹ (ìŠ¹/ë¬´/íŒ¨ ì˜ˆì¸¡)
# ============================================
def machine_learning(df: pd.DataFrame) -> Dict[str, Any]:
    """RandomForest ê¸°ë°˜ ìŠ¹/ë¬´/íŒ¨ ì˜ˆì¸¡ ëª¨ë¸"""
    print("\n" + "=" * 60)
    print("ğŸ¤– [STEP 5] ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ (RandomForest)")
    print("=" * 60)
    
    # 5-1. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = ['total_actions', 'total_passes', 'total_shots', 
                    'success_rate', 'pass_ratio', 'shot_ratio', 
                    'avg_x_position', 'unique_players']
    
    X = df[feature_cols].fillna(0)
    y = df['result']
    
    # 5-2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 5-3. ëª¨ë¸ í•™ìŠµ
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    model.fit(X_train, y_train)
    
    # 5-4. ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = model.predict(X_test)
    print("\n[ë¶„ë¥˜ ë¦¬í¬íŠ¸]")
    print(classification_report(y_test, y_pred))
    
    # 5-5. Feature Importance
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n[Feature Importance (Top 5)]")
    print(importance.head())
    
    # 5-6. ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Feature Importance
    sns.barplot(x='importance', y='feature', data=importance, palette='viridis', ax=axes[0])
    axes[0].set_title('ë³€ìˆ˜ ì¤‘ìš”ë„ (Feature Importance)', fontsize=12)
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred, labels=['Win', 'Draw', 'Lose'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Win', 'Draw', 'Lose'],
                yticklabels=['Win', 'Draw', 'Lose'], ax=axes[1])
    axes[1].set_title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)', fontsize=12)
    axes[1].set_xlabel('ì˜ˆì¸¡')
    axes[1].set_ylabel('ì‹¤ì œ')
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/ml_results.png", dpi=150)
    print(f"\nğŸ¨ ì €ì¥: {OUTPUT_PATH}/ml_results.png")
    
    return {'model': model, 'importance': importance}


# ============================================
# 6. ì¸ì‚¬ì´íŠ¸ ìš”ì•½
# ============================================
def generate_insights(basic_stats: Dict, corr: pd.DataFrame, adv_stats: Dict, ml_results: Dict):
    """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ [FINAL] ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ë¦¬í¬íŠ¸")
    print("=" * 60)
    
    insights = []
    
    # ì¸ì‚¬ì´íŠ¸ 1: ìŠ¹ë¦¬íŒ€ íŠ¹ì„±
    win_stats = basic_stats['by_result'].loc['Win']
    lose_stats = basic_stats['by_result'].loc['Lose']
    diff = (win_stats['success_rate'] - lose_stats['success_rate']) * 100
    insights.append(f"1. ìŠ¹ë¦¬íŒ€ì€ íŒ¨ë°°íŒ€ ëŒ€ë¹„ ì•¡ì…˜ ì„±ê³µë¥ ì´ í‰ê·  {diff:.1f}%p ë†’ìŒ")
    
    # ì¸ì‚¬ì´íŠ¸ 2: í†µê³„ì  ìœ ì˜ì„±
    if adv_stats['anova']['p_value'] < 0.05:
        insights.append("2. ìŠ¹/ë¬´/íŒ¨ ê·¸ë£¹ ê°„ ì„±ê³µë¥  ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•¨ (p < 0.05)")
    
    # ì¸ì‚¬ì´íŠ¸ 3: í•µì‹¬ ë³€ìˆ˜
    top_feature = ml_results['importance'].iloc[0]['feature']
    insights.append(f"3. ìŠ¹íŒ¨ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜: '{top_feature}'")
    
    # ì¸ì‚¬ì´íŠ¸ 4: ìƒê´€ê´€ê³„ ê¸°ë°˜
    insights.append("4. ê³µê²©ì  í¬ì§€ì…”ë‹(avg_x_position)ì´ ë†’ì„ìˆ˜ë¡ ìŠˆíŒ… ì‹œë„ê°€ ì¦ê°€í•˜ëŠ” ê²½í–¥")
    
    print("\nğŸ“‹ [í•µì‹¬ ì¸ì‚¬ì´íŠ¸]")
    for insight in insights:
        print(f"   {insight}")
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    with open(f"{OUTPUT_PATH}/analysis_insights.txt", "w", encoding="utf-8") as f:
        f.write("K-ë¦¬ê·¸ ë°ì´í„° ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸\n")
        f.write("=" * 50 + "\n\n")
        for insight in insights:
            f.write(insight + "\n")
    
    print(f"\nğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {OUTPUT_PATH}/analysis_insights.txt")
    print("\nâœ… ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


# ============================================
# MAIN
# ============================================
if __name__ == "__main__":
    try:
        # Step 0: ë°ì´í„° ë¡œë“œ
        match_info, raw_data = load_data()
        
        # Step 1: íŒŒìƒë³€ìˆ˜ ìƒì„±
        analysis_df = create_features(match_info, raw_data)
        
        # Step 2: ê¸°ë³¸ í†µê³„
        basic_stats = basic_statistics(analysis_df)
        
        # Step 3: ìƒê´€ê´€ê³„ ë¶„ì„
        corr_matrix = correlation_analysis(analysis_df)
        
        # Step 4: ê³ ê¸‰ í†µê³„ ë¶„ì„
        adv_stats = advanced_statistics(analysis_df)
        
        # Step 5: ë¨¸ì‹ ëŸ¬ë‹
        ml_results = machine_learning(analysis_df)
        
        # Step 6: ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        generate_insights(basic_stats, corr_matrix, adv_stats, ml_results)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
