"""
K-ë¦¬ê·¸ í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸
============================
[EDA â†’ í†µê³„ â†’ ML â†’ Causal AI â†’ ì‹œê³„ì—´ â†’ ë³´ê³ ì„œ]

ì „ì²´ ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ê³ 
HTML/Markdown ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

Author: Antigravity (Senior Data Analyst)
Date: 2026-01-21
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.inspection import permutation_importance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, r2_score, mean_squared_error
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.stats.proportion import proportions_ztest
from datetime import datetime
import warnings
import os
import json

warnings.filterwarnings('ignore')

# ============================================
# 0. í™˜ê²½ ì„¤ì •
# ============================================
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-whitegrid')

BASE_PATH = "/Users/sebokoh/ë°ì´í„°ë¶„ì„ì—°ìŠµ/ë°ì´ì½˜/kë¦¬ê·¸ë°ì´í„°/ë¦¬ê·¸ë°ì´í„°/epl_project/data"
OUTPUT_PATH = "/Users/sebokoh/ë°ì´í„°ë¶„ì„ì—°ìŠµ/ë°ì´ì½˜/kë¦¬ê·¸ë°ì´í„°/ë¦¬ê·¸ë°ì´í„°/epl_project/output"
REPORT_PATH = "/Users/sebokoh/ë°ì´í„°ë¶„ì„ì—°ìŠµ/ë°ì´ì½˜/kë¦¬ê·¸ë°ì´í„°/ë¦¬ê·¸ë°ì´í„°/epl_project/reports"
os.makedirs(OUTPUT_PATH, exist_ok=True)
os.makedirs(REPORT_PATH, exist_ok=True)

# ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
ANALYSIS_RESULTS = {
    'meta': {},
    'eda': {},
    'general_stats': {},  # ì¼ë°˜ í†µê³„ë¶„ì„ ê²°ê³¼
    'statistics': {},
    'ml': {},
    'causal': {},
    'timeseries': {},
    'insights': []
}


def print_header(title: str, emoji: str = "ğŸ“Š"):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 60)
    print(f"{emoji} {title}")
    print("=" * 60)


# ============================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================
def load_and_prepare_data():
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
    print_header("STEP 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬", "ğŸ“‚")

    match_info = pd.read_csv(f"{BASE_PATH}/match_info.csv")
    raw_data = pd.read_csv(f"{BASE_PATH}/raw_data.csv")

    # ë©”íƒ€ ì •ë³´ ì €ì¥
    ANALYSIS_RESULTS['meta'] = {
        'total_matches': match_info['game_id'].nunique(),
        'total_events': len(raw_data),
        'analysis_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'data_period': f"{match_info['game_date'].min()} ~ {match_info['game_date'].max()}"
    }

    print(f"âœ… ì´ ê²½ê¸° ìˆ˜: {ANALYSIS_RESULTS['meta']['total_matches']}")
    print(f"âœ… ì´ ì´ë²¤íŠ¸ ìˆ˜: {ANALYSIS_RESULTS['meta']['total_events']:,}")

    # ê²½ê¸°ë³„ ì§‘ê³„
    game_stats = raw_data.groupby(['game_id', 'team_id']).agg(
        total_actions=('action_id', 'count'),
        total_passes=('type_name', lambda x: (x == 'Pass').sum()),
        total_shots=('type_name', lambda x: (x == 'Shot').sum()),
        successful_actions=('result_name', lambda x: (x == 'Successful').sum()),
        avg_x_position=('start_x', 'mean'),
        avg_y_position=('start_y', 'mean'),
        unique_players=('player_id', 'nunique')
    ).reset_index()

    # íŒŒìƒë³€ìˆ˜
    game_stats['pass_ratio'] = game_stats['total_passes'] / game_stats['total_actions']
    game_stats['shot_ratio'] = game_stats['total_shots'] / game_stats['total_actions']
    game_stats['success_rate'] = game_stats['successful_actions'] / game_stats['total_actions']

    # ë©”íƒ€ë°ì´í„° ë³‘í•© ë° ê²°ê³¼ ë¼ë²¨ë§
    merged = game_stats.merge(
        match_info[['game_id', 'home_team_id', 'away_team_id', 'home_score', 'away_score',
                   'home_team_name_ko', 'away_team_name_ko']],
        on='game_id', how='left'
    )

    def get_result(row):
        if row['team_id'] == row['home_team_id']:
            if row['home_score'] > row['away_score']: return 'Win'
            elif row['home_score'] < row['away_score']: return 'Lose'
            else: return 'Draw'
        else:
            if row['away_score'] > row['home_score']: return 'Win'
            elif row['away_score'] < row['home_score']: return 'Lose'
            else: return 'Draw'

    merged['result'] = merged.apply(get_result, axis=1)
    merged['win'] = (merged['result'] == 'Win').astype(int)

    return match_info, raw_data, merged


# ============================================
# 2. EDA ë° ê¸°ë³¸ í†µê³„
# ============================================
def run_eda(df):
    """íƒìƒ‰ì  ë°ì´í„° ë¶„ì„"""
    print_header("STEP 2: EDA & ê¸°ë³¸ í†µê³„", "ğŸ“ˆ")

    numeric_cols = ['total_actions', 'total_passes', 'total_shots',
                    'success_rate', 'pass_ratio', 'shot_ratio']

    # ê¸°ìˆ í†µê³„
    desc_stats = df[numeric_cols].describe()

    # ê²°ê³¼ë³„ í†µê³„
    result_stats = df.groupby('result')[numeric_cols].mean()

    ANALYSIS_RESULTS['eda'] = {
        'mean_actions': df['total_actions'].mean(),
        'mean_passes': df['total_passes'].mean(),
        'mean_shots': df['total_shots'].mean(),
        'mean_success_rate': df['success_rate'].mean(),
        'win_rate': df['win'].mean()
    }

    print(f"âœ… ê²½ê¸°ë‹¹ í‰ê·  ì•¡ì…˜: {ANALYSIS_RESULTS['eda']['mean_actions']:.0f}")
    print(f"âœ… í‰ê·  ì„±ê³µë¥ : {ANALYSIS_RESULTS['eda']['mean_success_rate']:.1%}")

    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    for idx, col in enumerate(numeric_cols):
        sns.histplot(df[col], kde=True, ax=axes[idx], color='steelblue')
        axes[idx].set_title(f'{col} ë¶„í¬', fontsize=12)
        axes[idx].axvline(df[col].mean(), color='red', linestyle='--')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/01_eda_distributions.png", dpi=150)
    plt.close()

    return desc_stats, result_stats


# ============================================
# 2.5 ì¼ë°˜ í†µê³„ ë¶„ì„ (NEW)
# ============================================
def run_general_statistics(df, match_info):
    """ì¼ë°˜ í†µê³„ ë¶„ì„: t-ê²€ì •, A/Bí…ŒìŠ¤íŠ¸, íšŒê·€, ARIMA"""
    print_header("STEP 2.5: ì¼ë°˜ í†µê³„ ë¶„ì„", "ğŸ§ª")

    results = {}

    # í™ˆ/ì›ì • êµ¬ë¶„ ì¶”ê°€
    df['is_home'] = (df['team_id'] == df['home_team_id']).astype(int)
    df['game_date'] = pd.to_datetime(match_info.set_index('game_id').loc[df['game_id'], 'game_date'].values)

    # === 1. t-ê²€ì •: í™ˆ vs ì›ì • ===
    home_success = df[df['is_home'] == 1]['success_rate']
    away_success = df[df['is_home'] == 0]['success_rate']
    t_stat, p_ttest = stats.ttest_ind(home_success, away_success)

    results['ttest'] = {
        'home_mean': home_success.mean(),
        'away_mean': away_success.mean(),
        'p_value': p_ttest,
        'significant': p_ttest < 0.05
    }
    print(f"  [t-ê²€ì •] í™ˆ({home_success.mean():.3f}) vs ì›ì •({away_success.mean():.3f}), p={p_ttest:.4f}")

    # === 2. ì¹´ì´ì œê³±: í™ˆ/ì›ì • vs ìŠ¹íŒ¨ ===
    contingency = pd.crosstab(df['is_home'], df['win'])
    chi2, p_chi, _, _ = stats.chi2_contingency(contingency)

    results['chi2'] = {'chi2': chi2, 'p_value': p_chi, 'significant': p_chi < 0.05}
    print(f"  [ì¹´ì´ì œê³±] í™ˆ/ì›ì • â†” ìŠ¹íŒ¨ ë…ë¦½ì„±: Ï‡Â²={chi2:.2f}, p={p_chi:.4f}")

    # === 3. A/B í…ŒìŠ¤íŠ¸: ìŠˆíŒ… ìˆ˜ ===
    median_shots = df['total_shots'].median()
    group_a = df[df['total_shots'] >= median_shots]['win']
    group_b = df[df['total_shots'] < median_shots]['win']

    count = np.array([group_a.sum(), group_b.sum()])
    nobs = np.array([len(group_a), len(group_b)])
    z_stat, p_ab = proportions_ztest(count, nobs)
    lift = (group_a.mean() - group_b.mean()) / group_b.mean() * 100 if group_b.mean() > 0 else 0

    results['ab_test'] = {
        'high_shots_win_rate': group_a.mean(),
        'low_shots_win_rate': group_b.mean(),
        'lift_percent': lift,
        'p_value': p_ab,
        'significant': p_ab < 0.05
    }
    print(f"  [A/Bí…ŒìŠ¤íŠ¸] ë†’ì€ ìŠˆíŒ…({group_a.mean():.1%}) vs ë‚®ì€ ìŠˆíŒ…({group_b.mean():.1%}), Lift={lift:.1f}%")

    # === 4. ë¡œì§€ìŠ¤í‹± íšŒê·€ ===
    X_log = df[['total_shots', 'success_rate', 'avg_x_position', 'total_passes']].fillna(0)
    y_log = df['win']
    log_model = LogisticRegression(max_iter=1000)
    log_model.fit(X_log, y_log)
    log_accuracy = log_model.score(X_log, y_log)

    results['logistic'] = {
        'accuracy': log_accuracy,
        'odds_ratios': dict(zip(X_log.columns, np.exp(log_model.coef_[0])))
    }
    print(f"  [ë¡œì§€ìŠ¤í‹± íšŒê·€] ì •í™•ë„: {log_accuracy:.1%}")

    # === 5. ARIMA ì˜ˆì¸¡ ===
    try:
        daily = df.groupby('game_date')['success_rate'].mean().sort_index()
        daily = daily.asfreq('D', method='ffill')
        if len(daily) >= 30:
            arima_model = ARIMA(daily, order=(1,1,1))
            arima_fit = arima_model.fit()
            forecast = arima_fit.forecast(steps=7)
            results['arima'] = {'forecast_mean': forecast.mean(), 'aic': arima_fit.aic}
            print(f"  [ARIMA] 7ì¼ ì˜ˆì¸¡ í‰ê· : {forecast.mean():.3f}")
        else:
            results['arima'] = {'error': 'Insufficient data'}
    except Exception as e:
        results['arima'] = {'error': str(e)}

    # === 6. ì‹œê³„ì—´ ë¶„í•´ ===
    try:
        daily_actions = df.groupby('game_date')['total_actions'].mean().sort_index()
        daily_actions = daily_actions.asfreq('D', method='ffill')
        if len(daily_actions) >= 14:
            decomp = seasonal_decompose(daily_actions, model='additive', period=7)
            results['decomposition'] = {
                'trend_mean': decomp.trend.dropna().mean(),
                'seasonal_amplitude': decomp.seasonal.max() - decomp.seasonal.min()
            }
            print(f"  [ì‹œê³„ì—´ë¶„í•´] Trend í‰ê· : {decomp.trend.dropna().mean():.1f}")
        else:
            results['decomposition'] = {'error': 'Insufficient data'}
    except Exception as e:
        results['decomposition'] = {'error': str(e)}

    ANALYSIS_RESULTS['general_stats'] = results

    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # t-ê²€ì •
    ax1 = axes[0, 0]
    ax1.bar(['í™ˆíŒ€', 'ì›ì •íŒ€'], [home_success.mean(), away_success.mean()], color=['coral', 'steelblue'])
    ax1.set_ylabel('ì„±ê³µë¥ ')
    ax1.set_title(f't-ê²€ì •: í™ˆ vs ì›ì • (p={p_ttest:.4f})')

    # A/B í…ŒìŠ¤íŠ¸
    ax2 = axes[0, 1]
    ax2.bar(['ë†’ì€\nìŠˆíŒ…', 'ë‚®ì€\nìŠˆíŒ…'], [group_a.mean(), group_b.mean()],
            color=['seagreen' if p_ab < 0.05 else 'gray', 'lightcoral'])
    ax2.set_ylabel('ìŠ¹ë¥ ')
    ax2.set_title(f'A/Bí…ŒìŠ¤íŠ¸: ìŠˆíŒ…ìˆ˜ë³„ ìŠ¹ë¥  (Lift={lift:.1f}%)')

    # ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜
    ax3 = axes[1, 0]
    coefs = log_model.coef_[0]
    colors = ['seagreen' if c > 0 else 'lightcoral' for c in coefs]
    ax3.barh(X_log.columns, coefs, color=colors)
    ax3.axvline(x=0, color='black', linestyle='--')
    ax3.set_title(f'ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜ (ì •í™•ë„={log_accuracy:.1%})')

    # ì¹´ì´ì œê³±
    ax4 = axes[1, 1]
    contingency.plot(kind='bar', ax=ax4, color=['lightcoral', 'seagreen'])
    ax4.set_title(f'ì¹´ì´ì œê³±: í™ˆ/ì›ì • vs ìŠ¹íŒ¨ (p={p_chi:.4f})')
    ax4.set_xticklabels(['ì›ì •', 'í™ˆ'], rotation=0)
    ax4.legend(['íŒ¨ë°°', 'ìŠ¹ë¦¬'])

    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/02_5_general_stats.png", dpi=150)
    plt.close()
    print(f"  ğŸ¨ ì €ì¥: {OUTPUT_PATH}/02_5_general_stats.png")

    return results


# ============================================
# 3. ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================
def run_correlation(df):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    print_header("STEP 3: ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ”—")

    numeric_cols = ['total_actions', 'total_passes', 'total_shots',
                    'success_rate', 'pass_ratio', 'avg_x_position']

    corr_matrix = df[numeric_cols].corr()

    # ì£¼ìš” ìƒê´€ê´€ê³„ ì¶”ì¶œ
    high_corr = []
    for i in range(len(numeric_cols)):
        for j in range(i+1, len(numeric_cols)):
            if abs(corr_matrix.iloc[i, j]) > 0.5:
                high_corr.append({
                    'var1': numeric_cols[i],
                    'var2': numeric_cols[j],
                    'correlation': corr_matrix.iloc[i, j]
                })

    ANALYSIS_RESULTS['statistics']['high_correlations'] = high_corr

    # ì‹œê°í™”
    plt.figure(figsize=(10, 8))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r',
                center=0, fmt='.2f', square=True)
    plt.title('ìƒê´€ê´€ê³„ í–‰ë ¬', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/02_correlation_matrix.png", dpi=150)
    plt.close()

    print(f"âœ… ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬: {len(high_corr)}ê°œ")

    return corr_matrix


# ============================================
# 4. ê³ ê¸‰ í†µê³„ ë¶„ì„
# ============================================
def run_advanced_statistics(df):
    """ê°€ì„¤ê²€ì • ë° ê³ ê¸‰ í†µê³„"""
    print_header("STEP 4: ê³ ê¸‰ í†µê³„ ë¶„ì„", "ğŸ§ª")

    # ANOVA
    win_data = df[df['result'] == 'Win']['success_rate']
    draw_data = df[df['result'] == 'Draw']['success_rate']
    lose_data = df[df['result'] == 'Lose']['success_rate']

    f_stat, p_anova = stats.f_oneway(win_data, draw_data, lose_data)

    # Cohen's d
    cohens_d = (win_data.mean() - lose_data.mean()) / np.sqrt(
        ((win_data.std()**2) + (lose_data.std()**2)) / 2
    )

    ANALYSIS_RESULTS['statistics']['anova'] = {
        'f_statistic': f_stat,
        'p_value': p_anova,
        'significant': p_anova < 0.05
    }
    ANALYSIS_RESULTS['statistics']['cohens_d'] = cohens_d

    print(f"âœ… ANOVA p-value: {p_anova:.4f} ({'ìœ ì˜ë¯¸' if p_anova < 0.05 else 'ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ'})")
    print(f"âœ… Cohen's d: {cohens_d:.3f}")

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    sns.boxplot(x='result', y='success_rate', data=df, order=['Win', 'Draw', 'Lose'],
                palette='Set2', ax=axes[0])
    axes[0].set_title('ìŠ¹/ë¬´/íŒ¨ë³„ ì„±ê³µë¥  ë¶„í¬')
    sns.boxplot(x='result', y='total_shots', data=df, order=['Win', 'Draw', 'Lose'],
                palette='Set2', ax=axes[1])
    axes[1].set_title('ìŠ¹/ë¬´/íŒ¨ë³„ ìŠˆíŒ… íšŸìˆ˜')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/03_advanced_stats.png", dpi=150)
    plt.close()

    return {'f_stat': f_stat, 'p_value': p_anova, 'cohens_d': cohens_d}


# ============================================
# 5. ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„
# ============================================
def run_machine_learning(df):
    """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print_header("STEP 5: ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„", "ğŸ¤–")

    feature_cols = ['total_actions', 'total_passes', 'total_shots',
                    'success_rate', 'pass_ratio', 'shot_ratio',
                    'avg_x_position', 'unique_players']

    X = df[feature_cols].fillna(0).values
    y = df['result'].values

    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ëª¨ë¸ í•™ìŠµ
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)
    }

    best_model = None
    best_score = 0
    best_name = ""

    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        score = accuracy_score(y_test, model.predict(X_test_scaled))
        if score > best_score:
            best_score = score
            best_model = model
            best_name = name

    # Permutation Importance
    perm_importance = permutation_importance(best_model, X_test_scaled, y_test, n_repeats=30, random_state=42)
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': perm_importance.importances_mean
    }).sort_values('importance', ascending=False)

    ANALYSIS_RESULTS['ml'] = {
        'best_model': best_name,
        'accuracy': best_score,
        'top_features': importance_df.head(3).to_dict('records')
    }

    print(f"âœ… ìµœê³  ëª¨ë¸: {best_name} (ì •í™•ë„: {best_score:.1%})")

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis', ax=axes[0])
    axes[0].set_title('Feature Importance')

    cm = confusion_matrix(y_test, best_model.predict(X_test_scaled))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],
                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    axes[1].set_title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/04_ml_results.png", dpi=150)
    plt.close()

    return best_model, importance_df


# ============================================
# 6. Causal AI ë¶„ì„
# ============================================
def run_causal_analysis(df):
    """ì¸ê³¼ ê´€ê³„ ë¶„ì„"""
    print_header("STEP 6: Causal AI ë¶„ì„", "ğŸ”¬")

    # Treatment ì •ì˜
    median_x = df['avg_x_position'].median()
    df['aggressive'] = (df['avg_x_position'] > median_x).astype(int)

    # ë‹¨ìˆœ ë¹„êµ
    treated_win = df[df['aggressive'] == 1]['win'].mean()
    control_win = df[df['aggressive'] == 0]['win'].mean()
    naive_ate = treated_win - control_win

    # PSM
    confounders = ['total_passes', 'success_rate', 'unique_players']
    X = df[confounders].fillna(0)

    ps_model = LogisticRegression(max_iter=1000)
    ps_model.fit(X, df['aggressive'])
    df['ps'] = ps_model.predict_proba(X)[:, 1]

    treated_df = df[df['aggressive'] == 1]
    control_df = df[df['aggressive'] == 0]

    nn = NearestNeighbors(n_neighbors=1)
    nn.fit(control_df[['ps']])
    _, indices = nn.kneighbors(treated_df[['ps']])
    matched_control = control_df.iloc[indices.flatten()]

    att = treated_df['win'].mean() - matched_control['win'].mean()

    ANALYSIS_RESULTS['causal'] = {
        'naive_ate': naive_ate,
        'psm_att': att,
        'selection_bias': naive_ate - att
    }

    print(f"âœ… ë‹¨ìˆœ ë¹„êµ (Biased): {naive_ate:.3f}")
    print(f"âœ… PSM ì¸ê³¼ íš¨ê³¼ (ATT): {att:.3f}")

    # ì‹œê°í™”
    plt.figure(figsize=(10, 5))
    methods = ['ë‹¨ìˆœ ë¹„êµ\n(Biased)', 'PSM\n(Causal)']
    effects = [naive_ate, att]
    colors = ['lightcoral', 'seagreen']
    bars = plt.bar(methods, effects, color=colors)
    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    plt.ylabel('ìŠ¹ë¥  ì°¨ì´')
    plt.title('ë‹¨ìˆœ ë¹„êµ vs PSM ì¸ê³¼ íš¨ê³¼')
    for bar, eff in zip(bars, effects):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{eff:.3f}', ha='center')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/05_causal_analysis.png", dpi=150)
    plt.close()

    return att


# ============================================
# 7. ì‹œê³„ì—´ ë¶„ì„
# ============================================
def run_timeseries_analysis(raw_data, match_info, merged_df):
    """ì‹œê³„ì—´ ëª¨ë©˜í…€ ë¶„ì„"""
    print_header("STEP 7: ì‹œê³„ì—´ ë¶„ì„", "â±ï¸")

    # ì‹œê°„ ìœˆë„ìš° ìƒì„±
    raw_data['time_window'] = (raw_data['time_seconds'] // 300).astype(int)

    window_stats = raw_data.groupby(['game_id', 'team_id', 'period_id', 'time_window']).agg(
        action_count=('action_id', 'count'),
        pass_count=('type_name', lambda x: (x == 'Pass').sum()),
        shot_count=('type_name', lambda x: (x == 'Shot').sum()),
        avg_x=('start_x', 'mean')
    ).reset_index()

    window_stats['success_rate'] = window_stats['pass_count'] / window_stats['action_count']
    window_stats['attack_intensity'] = window_stats['avg_x'] / 100
    window_stats['momentum_score'] = window_stats['success_rate'] * window_stats['attack_intensity']

    # ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„±
    sequence_features = []
    for (game_id, team_id), group in window_stats.groupby(['game_id', 'team_id']):
        group = group.sort_values('time_window')
        first_half = group[group['period_id'] == 1]
        second_half = group[group['period_id'] == 2]

        features = {
            'game_id': game_id, 'team_id': team_id,
            'first_half_momentum': first_half['momentum_score'].mean() if len(first_half) > 0 else 0,
            'second_half_momentum': second_half['momentum_score'].mean() if len(second_half) > 0 else 0,
            'momentum_std': group['momentum_score'].std(),
            'late_intensity': group.tail(3)['action_count'].mean() if len(group) >= 3 else 0
        }
        sequence_features.append(features)

    seq_df = pd.DataFrame(sequence_features)
    seq_df = seq_df.merge(merged_df[['game_id', 'team_id', 'win']], on=['game_id', 'team_id'])

    # ëª¨ë¸ í•™ìŠµ
    feature_cols = ['first_half_momentum', 'second_half_momentum', 'momentum_std', 'late_intensity']
    X = seq_df[feature_cols].fillna(0)
    y = seq_df['win']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    accuracy = accuracy_score(y_test, model.predict(X_test))

    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    ANALYSIS_RESULTS['timeseries'] = {
        'accuracy': accuracy,
        'top_feature': importance.iloc[0]['feature']
    }

    print(f"âœ… ì‹œê³„ì—´ ëª¨ë¸ ì •í™•ë„: {accuracy:.1%}")
    print(f"âœ… í•µì‹¬ í”¼ì²˜: {ANALYSIS_RESULTS['timeseries']['top_feature']}")

    # ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    sns.barplot(x='importance', y='feature', data=importance, palette='viridis')
    plt.title('ì‹œê³„ì—´ í”¼ì²˜ ì¤‘ìš”ë„')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PATH}/06_timeseries_importance.png", dpi=150)
    plt.close()

    return accuracy


# ============================================
# 8. ì¸ì‚¬ì´íŠ¸ ìƒì„±
# ============================================
def generate_insights():
    """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    print_header("STEP 8: ì¸ì‚¬ì´íŠ¸ ìƒì„±", "ğŸ’¡")

    insights = []

    # ML ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
    ml_results = ANALYSIS_RESULTS['ml']
    insights.append({
        'category': 'ë¨¸ì‹ ëŸ¬ë‹',
        'finding': f"ìµœê³  ëª¨ë¸ {ml_results['best_model']}ì€ {ml_results['accuracy']:.1%} ì •í™•ë„ ë‹¬ì„±",
        'implication': f"ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” '{ml_results['top_features'][0]['feature']}'"
    })

    # Causal AI ì¸ì‚¬ì´íŠ¸
    causal_results = ANALYSIS_RESULTS['causal']
    if causal_results['psm_att'] < 0:
        insights.append({
            'category': 'ì¸ê³¼ë¶„ì„',
            'finding': 'ê³µê²©ì  í¬ì§€ì…”ë‹ë§Œìœ¼ë¡œëŠ” ìŠ¹ë¦¬ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ',
            'implication': "'íš¨ìœ¨ì ì¸ ì „ì§„'ì´ í•µì‹¬, ë‹¨ìˆœ ì „ì§„ì€ ì—­íš¨ê³¼"
        })

    # ì‹œê³„ì—´ ì¸ì‚¬ì´íŠ¸
    ts_results = ANALYSIS_RESULTS['timeseries']
    insights.append({
        'category': 'ì‹œê³„ì—´',
        'finding': f"ì‹œê³„ì—´ ëª¨ë¸ ì •í™•ë„ {ts_results['accuracy']:.1%}ë¡œ ê¸°ì¡´ ëŒ€ë¹„ í–¥ìƒ",
        'implication': f"'{ts_results['top_feature']}'ì´ ìŠ¹íŒ¨ì˜ ê²°ì •ì  ìš”ì†Œ"
    })

    # í†µê³„ ì¸ì‚¬ì´íŠ¸
    stats_results = ANALYSIS_RESULTS['statistics']
    if stats_results['anova']['significant']:
        insights.append({
            'category': 'í†µê³„',
            'finding': 'ìŠ¹/ë¬´/íŒ¨ ê·¸ë£¹ ê°„ ì„±ê³µë¥  ì°¨ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸',
            'implication': 'ì„±ê³µë¥  ê´€ë¦¬ê°€ ìŠ¹ë¦¬ì˜ í•µì‹¬ ì „ëµ'
        })

    ANALYSIS_RESULTS['insights'] = insights

    print(f"âœ… ì´ {len(insights)}ê°œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ")

    return insights


# ============================================
# 9. HTML ë³´ê³ ì„œ ìƒì„±
# ============================================
def generate_html_report():
    """HTML ë³´ê³ ì„œ ìƒì„±"""
    print_header("STEP 9: HTML ë³´ê³ ì„œ ìƒì„±", "ğŸ“„")

    html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>K-ë¦¬ê·¸ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'Apple SD Gothic Neo', sans-serif; background: #f5f7fa; color: #333; line-height: 1.8; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 40px 20px; }}
        header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 60px 40px; border-radius: 16px; margin-bottom: 40px; }}
        header h1 {{ font-size: 2.5rem; margin-bottom: 10px; }}
        header p {{ opacity: 0.9; font-size: 1.1rem; }}
        .meta {{ display: flex; gap: 30px; margin-top: 20px; flex-wrap: wrap; }}
        .meta-item {{ background: rgba(255,255,255,0.2); padding: 10px 20px; border-radius: 8px; }}
        section {{ background: white; border-radius: 16px; padding: 40px; margin-bottom: 30px; box-shadow: 0 4px 20px rgba(0,0,0,0.08); }}
        section h2 {{ color: #667eea; font-size: 1.5rem; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 2px solid #eee; }}
        .kpi-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .kpi-card {{ background: linear-gradient(135deg, #f5f7fa 0%, #e4e8ed 100%); padding: 25px; border-radius: 12px; text-align: center; }}
        .kpi-value {{ font-size: 2rem; font-weight: bold; color: #667eea; }}
        .kpi-label {{ color: #666; margin-top: 5px; }}
        .insight-card {{ background: #f8fafc; border-left: 4px solid #667eea; padding: 20px; margin: 15px 0; border-radius: 0 8px 8px 0; }}
        .insight-category {{ color: #667eea; font-weight: bold; font-size: 0.9rem; }}
        .insight-finding {{ font-size: 1.1rem; margin: 10px 0; }}
        .insight-implication {{ color: #666; font-style: italic; }}
        .chart-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px; margin: 20px 0; }}
        .chart-card {{ text-align: center; }}
        .chart-card img {{ max-width: 100%; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .chart-title {{ margin-top: 10px; color: #666; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px 15px; text-align: left; border-bottom: 1px solid #eee; }}
        th {{ background: #f8fafc; color: #667eea; }}
        tr:hover {{ background: #f8fafc; }}
        .conclusion {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; border-radius: 16px; }}
        .conclusion h2 {{ color: white; border-bottom-color: rgba(255,255,255,0.3); }}
        footer {{ text-align: center; padding: 40px; color: #999; }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>âš½ K-ë¦¬ê·¸ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ</h1>
            <p>Antigravity AI ë¶„ì„ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë¨</p>
            <div class="meta">
                <div class="meta-item">ğŸ“… ë¶„ì„ì¼ì‹œ: {ANALYSIS_RESULTS['meta']['analysis_date']}</div>
                <div class="meta-item">ğŸŸï¸ ì´ ê²½ê¸°: {ANALYSIS_RESULTS['meta']['total_matches']}ê²½ê¸°</div>
                <div class="meta-item">ğŸ“Š ì´ ì´ë²¤íŠ¸: {ANALYSIS_RESULTS['meta']['total_events']:,}ê±´</div>
            </div>
        </header>

        <section>
            <h2>ğŸ“Š 1. í•µì‹¬ ì§€í‘œ (KPI)</h2>
            <div class="kpi-grid">
                <div class="kpi-card">
                    <div class="kpi-value">{ANALYSIS_RESULTS['eda']['mean_actions']:.0f}</div>
                    <div class="kpi-label">ê²½ê¸°ë‹¹ í‰ê·  ì•¡ì…˜</div>
                </div>
                <div class="kpi-card">
                    <div class="kpi-value">{ANALYSIS_RESULTS['eda']['mean_success_rate']:.1%}</div>
                    <div class="kpi-label">í‰ê·  ì„±ê³µë¥ </div>
                </div>
                <div class="kpi-card">
                    <div class="kpi-value">{ANALYSIS_RESULTS['ml']['accuracy']:.1%}</div>
                    <div class="kpi-label">ML ì˜ˆì¸¡ ì •í™•ë„</div>
                </div>
                <div class="kpi-card">
                    <div class="kpi-value">{ANALYSIS_RESULTS['timeseries']['accuracy']:.1%}</div>
                    <div class="kpi-label">ì‹œê³„ì—´ ëª¨ë¸ ì •í™•ë„</div>
                </div>
            </div>
        </section>

        <section>
            <h2>ğŸ’¡ 2. í•µì‹¬ ì¸ì‚¬ì´íŠ¸</h2>
            {''.join([f'''
            <div class="insight-card">
                <div class="insight-category">{insight['category']}</div>
                <div class="insight-finding">{insight['finding']}</div>
                <div class="insight-implication">â†’ {insight['implication']}</div>
            </div>
            ''' for insight in ANALYSIS_RESULTS['insights']])}
        </section>

        <section>
            <h2>ğŸ“ˆ 3. ë¶„ì„ ì‹œê°í™”</h2>
            <div class="chart-grid">
                <div class="chart-card">
                    <img src="../output/02_correlation_matrix.png" alt="ìƒê´€ê´€ê³„">
                    <div class="chart-title">ìƒê´€ê´€ê³„ í–‰ë ¬</div>
                </div>
                <div class="chart-card">
                    <img src="../output/04_ml_results.png" alt="ML ê²°ê³¼">
                    <div class="chart-title">ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ ê²°ê³¼</div>
                </div>
                <div class="chart-card">
                    <img src="../output/05_causal_analysis.png" alt="ì¸ê³¼ë¶„ì„">
                    <div class="chart-title">Causal AI ë¶„ì„</div>
                </div>
                <div class="chart-card">
                    <img src="../output/06_timeseries_importance.png" alt="ì‹œê³„ì—´">
                    <div class="chart-title">ì‹œê³„ì—´ í”¼ì²˜ ì¤‘ìš”ë„</div>
                </div>
            </div>
        </section>

        <section>
            <h2>ğŸ”¬ 4. Causal AI ë¶„ì„ ê²°ê³¼</h2>
            <p><strong>ì—°êµ¬ ì§ˆë¬¸:</strong> "ê³µê²©ì  í¬ì§€ì…”ë‹ì´ ì‹¤ì œë¡œ ìŠ¹ë¦¬ë¥¼ ìœ ë°œí•˜ëŠ”ê°€?"</p>
            <table>
                <tr><th>ë¶„ì„ ë°©ë²•</th><th>íš¨ê³¼ í¬ê¸°</th><th>í•´ì„</th></tr>
                <tr><td>ë‹¨ìˆœ ë¹„êµ (Biased)</td><td>{ANALYSIS_RESULTS['causal']['naive_ate']:.3f}</td><td>Selection Bias í¬í•¨</td></tr>
                <tr><td>PSM (Causal)</td><td>{ANALYSIS_RESULTS['causal']['psm_att']:.3f}</td><td>ì¸ê³¼ì  íš¨ê³¼ ì¶”ì •</td></tr>
                <tr><td>Selection Bias</td><td>{ANALYSIS_RESULTS['causal']['selection_bias']:.3f}</td><td>í¸í–¥ í¬ê¸°</td></tr>
            </table>
        </section>

        <section class="conclusion">
            <h2>ğŸ¯ 5. ê²°ë¡  ë° ì œì–¸</h2>
            <ul style="margin-left: 20px;">
                <li><strong>íš¨ìœ¨ì  ì „ì§„ì´ í•µì‹¬:</strong> ë‹¨ìˆœíˆ ê³µê²©ì ìœ¼ë¡œ ì§„ì¶œí•˜ëŠ” ê²ƒë³´ë‹¤ ì„±ê³µë¥  ë†’ì€ í”Œë ˆì´ê°€ ìŠ¹ë¦¬ì™€ ì—°ê²°ë¨</li>
                <li><strong>í›„ë°˜ì „ ëª¨ë©˜í…€ ê´€ë¦¬:</strong> ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼, í›„ë°˜ì „ íë¦„ ìœ ì§€ê°€ ìŠ¹íŒ¨ì˜ ë¶„ìˆ˜ë ¹</li>
                <li><strong>ë°ì´í„° ê¸°ë°˜ ì „ìˆ :</strong> ìƒëŒ€íŒ€ ë¶„ì„ ì‹œ ì„±ê³µë¥ ê³¼ í‰ê·  í¬ì§€ì…˜ ë°ì´í„° í™œìš© ê¶Œì¥</li>
            </ul>
        </section>

        <footer>
            <p>Generated by Antigravity AI Analysis System | K-League Data Analytics</p>
            <p>ë¶„ì„ ì¼ì‹œ: {ANALYSIS_RESULTS['meta']['analysis_date']}</p>
        </footer>
    </div>
</body>
</html>
"""

    report_path = f"{REPORT_PATH}/k_league_analysis_report.html"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"âœ… HTML ë³´ê³ ì„œ ì €ì¥: {report_path}")

    return report_path


# ============================================
# 10. Markdown ë³´ê³ ì„œ ìƒì„±
# ============================================
def generate_markdown_report():
    """Markdown ë³´ê³ ì„œ ìƒì„±"""

    md_content = f"""# âš½ K-ë¦¬ê·¸ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ì¼ì‹œ:** {ANALYSIS_RESULTS['meta']['analysis_date']}
**ë¶„ì„ ì‹œìŠ¤í…œ:** Antigravity AI

---

## ğŸ“Š 1. ë¶„ì„ ê°œìš”

| í•­ëª© | ê°’ |
|------|-----|
| ì´ ê²½ê¸° ìˆ˜ | {ANALYSIS_RESULTS['meta']['total_matches']}ê²½ê¸° |
| ì´ ì´ë²¤íŠ¸ ìˆ˜ | {ANALYSIS_RESULTS['meta']['total_events']:,}ê±´ |
| ë°ì´í„° ê¸°ê°„ | {ANALYSIS_RESULTS['meta']['data_period']} |

---

## ğŸ“ˆ 2. í•µì‹¬ ì§€í‘œ (KPI)

| ì§€í‘œ | ê°’ |
|------|-----|
| ê²½ê¸°ë‹¹ í‰ê·  ì•¡ì…˜ | {ANALYSIS_RESULTS['eda']['mean_actions']:.0f} |
| í‰ê·  ì„±ê³µë¥  | {ANALYSIS_RESULTS['eda']['mean_success_rate']:.1%} |
| ML ì˜ˆì¸¡ ì •í™•ë„ | {ANALYSIS_RESULTS['ml']['accuracy']:.1%} |
| ì‹œê³„ì—´ ëª¨ë¸ ì •í™•ë„ | {ANALYSIS_RESULTS['timeseries']['accuracy']:.1%} |

---

## ğŸ’¡ 3. í•µì‹¬ ì¸ì‚¬ì´íŠ¸

{''.join([f'''
### {insight['category']}
- **ë°œê²¬:** {insight['finding']}
- **ì‹œì‚¬ì :** {insight['implication']}

''' for insight in ANALYSIS_RESULTS['insights']])}

---

## ğŸ”¬ 4. Causal AI ë¶„ì„

**ì—°êµ¬ ì§ˆë¬¸:** "ê³µê²©ì  í¬ì§€ì…”ë‹ì´ ì‹¤ì œë¡œ ìŠ¹ë¦¬ë¥¼ ìœ ë°œí•˜ëŠ”ê°€?"

| ë¶„ì„ ë°©ë²• | íš¨ê³¼ í¬ê¸° | í•´ì„ |
|----------|----------|------|
| ë‹¨ìˆœ ë¹„êµ | {ANALYSIS_RESULTS['causal']['naive_ate']:.3f} | Selection Bias í¬í•¨ |
| PSM (ì¸ê³¼) | {ANALYSIS_RESULTS['causal']['psm_att']:.3f} | ì¸ê³¼ì  íš¨ê³¼ |
| í¸í–¥ í¬ê¸° | {ANALYSIS_RESULTS['causal']['selection_bias']:.3f} | - |

---

## ğŸ¯ 5. ê²°ë¡  ë° ì œì–¸

1. **íš¨ìœ¨ì  ì „ì§„ì´ í•µì‹¬**: ë‹¨ìˆœ ê³µê²©ë³´ë‹¤ ì„±ê³µë¥  ë†’ì€ í”Œë ˆì´ê°€ ìŠ¹ë¦¬ ì—°ê²°
2. **í›„ë°˜ì „ ëª¨ë©˜í…€**: ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼, í›„ë°˜ì „ íë¦„ ìœ ì§€ê°€ ìŠ¹íŒ¨ ë¶„ìˆ˜ë ¹
3. **ë°ì´í„° ê¸°ë°˜ ì „ìˆ **: ìƒëŒ€íŒ€ ë¶„ì„ ì‹œ ì„±ê³µë¥ ê³¼ í‰ê·  í¬ì§€ì…˜ í™œìš© ê¶Œì¥

---

*Generated by Antigravity AI Analysis System*
"""

    md_path = f"{REPORT_PATH}/k_league_analysis_report.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"âœ… Markdown ë³´ê³ ì„œ ì €ì¥: {md_path}")

    return md_path


# ============================================
# MAIN PIPELINE
# ============================================
def main():
    """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    start_time = datetime.now()

    print("\n" + "ğŸš€" * 25)
    print("   K-ë¦¬ê·¸ í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    print("   [EDA â†’ í†µê³„ â†’ ML â†’ Causal AI â†’ ì‹œê³„ì—´ â†’ ë³´ê³ ì„œ]")
    print("ğŸš€" * 25 + "\n")

    try:
        # 1. ë°ì´í„° ë¡œë“œ
        match_info, raw_data, df = load_and_prepare_data()

        # 2. EDA
        run_eda(df)

        # 2.5 ì¼ë°˜ í†µê³„ ë¶„ì„ (NEW)
        run_general_statistics(df, match_info)

        # 3. ìƒê´€ê´€ê³„
        run_correlation(df)

        # 4. ê³ ê¸‰ í†µê³„
        run_advanced_statistics(df)

        # 5. ë¨¸ì‹ ëŸ¬ë‹
        run_machine_learning(df)

        # 6. Causal AI
        run_causal_analysis(df)

        # 7. ì‹œê³„ì—´
        run_timeseries_analysis(raw_data, match_info, df)

        # 8. ì¸ì‚¬ì´íŠ¸
        generate_insights()

        # 9. HTML ë³´ê³ ì„œ
        html_path = generate_html_report()

        # 10. Markdown ë³´ê³ ì„œ
        md_path = generate_markdown_report()

        # ê²°ê³¼ JSON ì €ì¥
        with open(f"{REPORT_PATH}/analysis_results.json", 'w', encoding='utf-8') as f:
            # Convert non-serializable items
            results_serializable = ANALYSIS_RESULTS.copy()
            results_serializable['ml']['top_features'] = [dict(x) for x in results_serializable['ml']['top_features']]
            json.dump(results_serializable, f, ensure_ascii=False, indent=2, default=str)

        elapsed = (datetime.now() - start_time).total_seconds()

        print("\n" + "=" * 60)
        print("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("=" * 60)
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ğŸ“„ HTML ë³´ê³ ì„œ: {html_path}")
        print(f"ğŸ“„ Markdown ë³´ê³ ì„œ: {md_path}")
        print(f"ğŸ“Š ì‹œê°í™” íŒŒì¼: {OUTPUT_PATH}/")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
