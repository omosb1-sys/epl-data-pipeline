"""
SLM Manager: í•˜ë“œì›¨ì–´ ë§ì¶¤í˜• ì†Œí˜• ëª¨ë¸ ê´€ë¦¬ì
================================================
8GB RAM ë§¥ í™˜ê²½ì„ ìœ„í•œ ì´ˆê²½ëŸ‰ ëª¨ë¸ ë¼ìš°íŒ… ë° ìµœì í™”

ì£¼ê¸°ëŠ¥:
1. í•˜ë“œì›¨ì–´ ê°ì§€: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸ í›„ ìµœì  ëª¨ë¸ í‹°ì–´ ê²°ì •
2. ëª¨ë¸ ë¼ìš°íŒ…: 8GB RAMì¼ ê²½ìš° 1.5B~3B ëª¨ë¸ ìš°ì„  ë°°ì •
3. ê°€ì† í™•ì¸: Metal ê°€ì† ì—¬ë¶€ ì²´í¬ ë° ì•ˆë‚´

Author: Antigravity (Hardware-Aware Architect)
Date: 2026-01-23
"""

import os
import psutil
import subprocess
import json

class SLMManager:
    """í•˜ë“œì›¨ì–´ ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ SLMì„ ì„ íƒí•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤."""
    
    def __init__(self):
        self.total_ram_gb = psutil.virtual_memory().total / (1024**3)
        self.arch = subprocess.run(['uname', '-m'], capture_output=True, text=True).stdout.strip()
        self.is_intel = self.arch == 'x86_64'
        if os.uname().sysname == 'Darwin':
            try:
                self.cpu_brand = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True).stdout.strip()
            except FileNotFoundError:
                self.cpu_brand = "Apple Silicon (Unknown)"
        else:
            self.cpu_brand = "Linux Generic Server"
        
    def get_optimal_model(self) -> str:
        """í˜„ì¬ ì‹œìŠ¤í…œì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.is_intel:
            # Intel Mac: GPU ê°€ì†ì´ ì•½í•˜ë¯€ë¡œ 0.5B ~ 1.5B ì´ˆëŒ€í˜• ëª¨ë¸ ì¶”ì²œ
            return "qwen2.5:0.5b" if self.total_ram_gb <= 8.5 else "qwen2.5:1.5b"
        
        if self.total_ram_gb <= 8.5:
            return "qwen2.5:1.5b"
        elif self.total_ram_gb <= 16.5:
            return "qwen2.5:7b"
        else:
            return "llama3.1:8b"

    def get_optimal_embedding_model(self) -> str:
        """[Unsloth SOTA] í•˜ë“œì›¨ì–´ ì„±ëŠ¥ê³¼ Fine-tuning íš¨ìœ¨ì„ ê³ ë ¤í•œ ì„ë² ë”© ëª¨ë¸ ì¶”ì²œ"""
        if self.is_intel:
            # Intel Mac (8GB): Unsloth ìµœì í™” ë² ì´ìŠ¤ë¼ì¸ì¸ MiniLM ì¶”ì²œ
            return "sentence-transformers/all-MiniLM-L6-v2"
        else:
            # Apple Silicon: Unslothê°€ 2ë°° ì´ìƒ ê°€ì†í•˜ëŠ” ModernBERT ë˜ëŠ” BGE-M3 ì¶”ì²œ
            if self.total_ram_gb > 16:
                return "BAAI/bge-m3" # ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ ì„ë² ë”©
            return "answerdotai/ModernBERT-large" # ìµœì‹  SOTA (Speed 2.2x up)

    def check_ollama_status(self):
        """ë¡œì»¬ Ollama ë° ëª¨ë¸ ì„¤ì¹˜ ìƒíƒœë¥¼ ì ê²€í•©ë‹ˆë‹¤."""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode != 0:
                return False, "Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤."
            
            models = result.stdout
            optimal = self.get_optimal_model()
            
            if optimal in models:
                return True, f"ìµœì ì˜ ëª¨ë¸({optimal})ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                return False, f"ìµœì  ëª¨ë¸({optimal})ì´ ì—†ìŠµë‹ˆë‹¤. 'ollama pull {optimal}'ì´ í•„ìš”í•©ë‹ˆë‹¤."
        except FileNotFoundError:
            return False, "Ollama ì»¤ë§¨ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def get_hardware_report(self):
        """ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  í•˜ë“œì›¨ì–´ ìµœì í™” ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        advice = ""
        if self.is_intel:
            advice = (f"í˜„ì¬ {self.cpu_brand} (MacBook Air 2017) í™˜ê²½ì…ë‹ˆë‹¤. "
                     "ì´ ì‚¬ì–‘ì€ Dual-Core CPUì™€ DDR3 ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ 0.5B ëª¨ë¸ì´ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤. "
                     "STEM ì›ì¹™ì— ë”°ë¼, ëŒ€í˜• ì„ë² ë”©ì€ RAMì— ì˜¤í”„ë¡œë“œí•˜ê³  CPU ì“°ë ˆë“œ ìˆ˜ë¥¼ 2~3ê°œë¡œ ì œí•œí•˜ì—¬ "
                     "ë°œì—´ê³¼ ë³‘ëª© í˜„ìƒì„ ë°©ì§€í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡  ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            advice = "Apple Silicon í™˜ê²½ì…ë‹ˆë‹¤. Metal ê°€ì†ì„ í†µí•´ 1.5B ëª¨ë¸ì„ ë§¤ìš° ë¯¼ì²©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        report = {
            "Total RAM": f"{self.total_ram_gb:.1f} GB",
            "Architecture": self.arch,
            "Target Model": self.get_optimal_model(),
            "Advice": advice
        }
        return report

    def query(self, prompt: str, system_prompt: str = "You are a helpful assistant.", temperature: float = 0.7) -> str:
        """Ollama APIë¥¼ í†µí•´ ë¡œì»¬ ëª¨ë¸ì—ê²Œ ì§ˆì˜í•©ë‹ˆë‹¤."""
        import requests
        
        model = self.get_optimal_model()
        url = "http://localhost:11434/api/generate"
        payload = {
            "model": model,
            "prompt": prompt,
            "system": system_prompt,
            "stream": False,
            "options": {
                "temperature": temperature
            }
        }
        
        try:
            response = requests.post(url, json=payload, timeout=30)
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            else:
                return f"Error: API returned status {response.status_code}"
        except Exception as e:
            return f"Error: {str(e)}"

if __name__ == "__main__":
    manager = SLMManager()
    print("ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ì§„ë‹¨ ê²°ê³¼:")
    print(json.dumps(manager.get_hardware_report(), indent=2, ensure_ascii=False))
    
    status, msg = manager.check_ollama_status()
    print(f"\nâœ… ìƒíƒœ ì ê²€: {msg}")
