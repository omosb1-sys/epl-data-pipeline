# [Skill] AI 응답 최적화 가이드 (나만의 프롬프트 요령)

## 1. 개요
이 문서는 사용자가 선호하는 AI 답변 스타일과 프롬프트 엔지니어링 팁을 정리한 지침서입니다. AI는 모든 응답 시 이 문서를 자신의 '행동 강령'으로 간주해야 합니다.

## 2. 핵심 지침 (주니어님의 노션 비법들)
*여기에 노션에 적어두신 내용들을 하나씩 적어주세요! 예시는 다음과 같습니다:*

1.  **가독성 우선:** 모든 답변은 읽기 편하게 '헤더(#)'와 '불렛포인트(-)'를 적극적으로 활용할 것.
2.  **친절한 설명:** 30년차 시니어가 3개월차 신입에게 가르치듯, 원리부터 차근차근 설명할 것.
3.  **코드 품질:** Python 코드를 제공할 때는 한글 주석을 상세히 달고, 바로 실행 가능한 형태로 제공할 것.
4.  **자동화 마인드:** 반복되는 작업은 함수(Function)로 만들 것을 먼저 제안할 것.
5.  *(추가: 노션에 있던 주니어님만의 프롬프트 팁들을 여기에 더 적어주세요!)*

## 3. 답변 금지 사항
- 너무 딱딱하거나 기계적인 말투는 지양할 것.
- 질문의 맥락을 생략하고 결과만 툭 던지지 말 것.

## 4. AI 적용 가이드
에이전트는 이 파일을 자신의 시스템 프롬프트의 일부로 인식하고, 사용자의 명시적인 요청이 없더라도 위 지침들을 모든 답변에 적용한다.





# 데이터 분석가를 위한 프롬프트 스킬 세트

## 📊 1. 데이터 탐색 및 이해 단계

### 1-1. 데이터셋 첫 분석
```
30년차 시니어 데이터분석가로서, 첨부한 [데이터명] 데이터를 분석해줘.
다음 내용을 포함해서:
1. 데이터의 구조 (행/열 개수, 컬럼명)
2. 각 컬럼의 데이터 타입과 의미
3. 결측치 및 이상치 여부
4. 기초 통계량 (평균, 중앙값, 표준편차 등)
5. 초보자가 주의해야 할 데이터 특징
```

### 1-2. 데이터 품질 체크
```
이 데이터의 품질을 전문가 관점에서 평가해줘.
체크해야 할 항목:
- 중복 데이터
- 이상치 (outlier)
- 데이터 불균형
- 컬럼 간 상관관계 문제
그리고 각 문제에 대한 해결 방법도 알려줘.
```

## 📈 2. 분석 방법론 선택

### 2-1. 통계분석 vs 머신러닝 판단
```
30년차 전문가로서, 이 [비즈니스 문제/데이터]에 통계분석과 머신러닝 중 어느 것이 적합한지 판단해줘.

비교 분석:
1. 통계분석 접근법
   - 적합한 이유
   - 사용할 통계 기법
   - 장단점

2. 머신러닝 접근법
   - 적합한 이유
   - 사용할 ML 알고리즘
   - 장단점

3. 최종 추천
   - 데이터 특성 고려
   - 비즈니스 목표 고려
   - 리소스(시간/컴퓨팅) 고려

4. 초보자 가이드
   - 어떤 경우에 통계분석을 쓰는지
   - 어떤 경우에 머신러닝을 쓰는지
```

### 2-2. 분석 목표 설정
```
[비즈니스 목표 또는 질문]을 달성하기 위해 이 데이터를 어떻게 분석하면 좋을까?
1. 분석 방향 3가지 제시
2. 각 방향의 장단점
3. 초보자에게 가장 적합한 접근법
4. 단계별 분석 로드맵
```

### 2-3. 가설 수립
```
이 데이터로 검증할 수 있는 의미있는 가설 5가지를 제시해줘.
각 가설에 대해:
- 검증 방법
- 필요한 분석 기법
- 예상되는 인사이트
```

## 🔍 3. 구체적 분석 요청

### 3-1. 탐색적 데이터 분석 (EDA)
```
30년차 전문가로서 이 데이터에 대한 탐색적 데이터 분석(EDA)을 수행해줘.
포함 내용:
1. 주요 변수들의 분포 분석
2. 변수 간 관계 파악
3. 패턴 및 트렌드 발견
4. 시각화 추천 (어떤 그래프가 적합한지)
5. 초보자를 위한 해석 가이드
```

### 3-2. 세그먼트 분석
```
이 데이터를 의미있는 그룹으로 세그먼트화해줘.
- 세그먼트 기준 추천
- 각 세그먼트의 특성
- 비즈니스 인사이트
- Python/SQL 코드 예시
```

## 💻 4. 코드 작성 지원

### 4-1. 데이터 전처리 및 기본 정제
```
이 데이터를 분석하기 위한 Python 전처리 코드를 작성해줘.
포함 사항:
- 라이브러리 import 및 데이터 로딩
- 결측치 처리 (삭제/대체 전략)
- 이상치 처리 (IQR/Z-score 기준)
- 데이터 타입 최적화 (Memory Usage 감소)
- 각 단계의 논리적 이유 설명
```

### 4-2. 파생 변수 생성 (Feature Engineering)
```
30년차 전문가로서 이 데이터에서 모델의 성능을 극대화할 수 있는 파생 변수(Derived Features)를 설계하고 생성 코드를 작성해줘.

다음 기법들을 적용해서:
1. 시간/날짜 기반 변수: (예: 요일, 시간대, 휴일 여부, 계절성)
2. 시계열 이동/평균 변수: (예: Lag features, Rolling window mean/std)
3. 수치형 변수 결합: (예: 변수 간 비율, 차이, 곱셈 피처)
4. 범주형 변수 인코딩: (예: Target encoding, Frequency encoding)
5. 도메인 지식 기반 변수: [데이터의 도메인, 예: 축구]를 고려한 핵심 지표 생성
6. 변수 선택(Feature Selection)에 대한 제언
```

### 4-2. 시각화 코드 및 대량 데이터 처리 (HoloViews/Datashader)
```
[원하는 분석]을 시각화하는 Python 코드를 작성해줘. 
특히 데이터가 방대할 경우를 대비해 HoloViews와 Datashader를 활용한 고성능 시각화 코드를 포함해줘.

요청 사항:
1. Matplotlib/Seaborn을 활용한 정적 시각화
2. Plotly를 활용한 인터랙티브 시각화
3. HoloViews/Bokeh를 활용한 대규모 데이터 대시보드 구성 (Zoom, Pan 기능 포함)
4. 차트별 핵심 인사이트 해석 가이드
```

### 4-3. 고성능 데이터 추출 및 처리 (DuckDB + Polars)
```
내 맥(Mac)의 자원을 최대한 활용하여 대용량 데이터를 지연 없이 빠르게 처리할 수 있도록 DuckDB와 Polars를 결합한 하이브리드 코드를 작성해줘.

요청 워크플로우:
1. DuckDB 엔진: 수백만 행의 로우 데이터(CSV/Parquet/SQL)에서 필요한 부분만 SQL로 초고속 추출.
2. Polars 전환: 추출된 데이터를 Polars DataFrame으로 변환하여 병렬 처리 기반의 신속한 전처리 실행.
3. Python/Pandas 호환: 고부하 처리가 끝난 후, 최종 머신러닝이나 통계 분석을 위해 익숙한 Pandas DataFrame으로 자연스럽게 전환.
4. 최신 팁: 복잡한 Python 루프 대신 DuckDB의 SQL 연산과 Polars의 Lazy API를 활용하여 속도를 10배 이상 높이는 방법 포함.
```

### 4-4. 초고속 데이터 포맷 활용 (Apache Parquet & Arrow)
```
CSV보다 100배 빠른 처리가 가능한 Apache Parquet 포맷과 메모리 내 데이터 이동의 표준인 Apache Arrow를 활용하여 데이터 파이프라인을 최적화해줘.

최적화 전략:
1. Parquet 전환: 용량이 큰 CSV 데이터를 컬럼 기반 저장 방식인 Parquet로 변환하는 코드 작성 (용량 1/5 감소, 읽기 속도 폭증).
2. Arrow 메모리 브릿지: DuckDB에서 추출한 데이터를 별도의 직렬화 없이 Apache Arrow 메모리 포맷을 통해 Polars나 Pandas로 즉시 전송 (Zero-copy).
3. 필터 푸시다운 (Filter Pushdown): 필요한 컬럼과 조건만 파일 레벨에서 걸러내어 메모리 사용량을 최소화하는 기법 적용.
4. 효과 설명: 왜 이 방식이 내 맥(Mac)의 SSD 수명을 보호하고 분석 속도를 혁신적으로 높이는지 시니어 관점에서 요약.
```

### 4-5. SQL 쿼리 작성 (DuckDB 특화)
```
DuckDB 환경에서 실행할 고속 분석 SQL 쿼리를 작성해줘.
- 대용량 데이터 직접 읽기 (read_parquet 등) 활용
- 복잡한 집계 및 윈도우 함수 성능 최적화
- DuckDB의 강력한 공간/시간 절약 기능 설명
- 최종 결과를 Arrow/Polars로 넘기는 브릿지 코드 포함
```

## 📊 5. 통계 분석

### 5-1. 기초 통계 분석
```
이 데이터에 대한 통계 분석을 수행해줘.
1. 기술통계량 계산 및 해석
2. 분포 검정 (정규성 등)
3. 상관관계 분석
4. 통계적 유의성 검정
통계 지식이 부족한 초보자도 이해할 수 있게 설명해줘.
```

### 5-2. A/B 테스트 분석
```
A/B 테스트 결과를 분석해줘.
- 적절한 통계 검정 방법
- 유의수준 및 검정력 설정
- 결과 해석
- 비즈니스 의사결정을 위한 제언
```

### 5-3. 고급 통계 분석
```
[분석 목표]를 위한 고급 통계 분석을 설계해줘.

필요한 내용:
1. 적합한 통계 기법 선택
   - 회귀분석 (선형/로지스틱/다중)
   - 분산분석 (ANOVA)
   - 시계열 분석
   - 생존분석 등

2. 가정 검증
   - 각 기법의 전제조건
   - 가정 위반 시 대안

3. 실행 코드 (Python/R)
4. 결과 해석 가이드
5. 보고서 작성 템플릿
```

## 🤖 6. 머신러닝 분석

### 6-1. ML 알고리즘 선택
```
[예측/분류/군집화] 문제를 해결하기 위한 머신러닝 접근법을 설계해줘.

1. 문제 유형 분석
   - 지도학습 vs 비지도학습
   - 회귀 vs 분류

2. 추천 알고리즘 3가지
   - 각 알고리즘의 특징
   - 장단점 비교
   - 데이터 요구사항

3. 초보자를 위한 시작 알고리즘
4. 단계별 구현 가이드
```

### 6-2. 모델 개발 프로세스
```
머신러닝 모델을 처음부터 개발하는 전체 프로세스를 알려줘.

단계별 가이드:
1. 데이터 준비
   - Train/Validation/Test 분할
   - 피처 엔지니어링
   - 스케일링/정규화

2. 모델 학습
   - 베이스라인 모델
   - 하이퍼파라미터 튜닝
   - 교차검증

3. 모델 평가
   - 적절한 평가지표 선택
   - 과적합/과소적합 진단
   - 모델 비교

4. 초보자가 흔히 하는 실수와 해결법
5. Python 코드 예시 (scikit-learn 기반)
```

### 6-3. 모델 해석 및 설명 (XAI - SHAP)
```
30년차 전문가로서, 학습된 머신러닝 모델에 대해 SHAP(SHapley Additive exPlanations)을 활용한 상세 해석을 수행해줘.

포함 내용:
1. SHAP Summary Plot 해석 (전체 피처의 영향력)
2. SHAP Force Plot 또는 Waterfall Plot 해석 (개별 샘플의 예측 원인)
3. SHAP Dependence Plot을 통한 피처 간 상호작용 분석
4. 모델의 '블랙박스' 로직을 비즈니스 언어로 번역
5. SHAP 지표 기반 모델 개선 방향 제안
```

### 6-4. 딥러닝 기반 시계열 분석 (PyTorch)
```
[데이터명] 시계열 데이터를 처리하기 위해 PyTorch 기반의 딥러닝 모델(예: LSTM, GRU, Transformers)을 설계하고 코드를 작성해줘.

요청 사항:
1. 데이터 전처리 (Sliding Window 생성, Scaling)
2. PyTorch Dataset 및 DataLoader 구현
3. 모델 아키텍처 설계 및 설명
4. 학습 루프 (Training Loop) 및 조기 종료(Early Stopping) 구현
5. 테스트 데이터에 대한 예측 및 성능 평가 (MSE, MAE)
```

### 6-5. 하이엔드 시계열 예측 (TimesFM)
```
Google의 TimesFM(Time-series Foundation Model)을 사용하여 [데이터명]에 대한 제로샷(Zero-shot) 예측을 수행하는 코드를 작성해줘.

포함 내용:
1. TimesFM 라이브러리 로드 및 모델 초기화
2. 컨텍스트 길이(Context Length)와 예측 길이(Horizon) 설정 가이드
3. 시각화를 포함한 예측 결과 도출
4. 기존 통계 모델(ARIMA 등)과의 성능 비교 포인트 설명
```

### 6-6. 인과관계 분석 (Causal AI)
```
[변수 A]와 [변수 B] 사이의 관계를 단순 상관관계를 넘어 Causal AI 관점에서 분석해줘.

분석 단계:
1. 인과 그래프(Causal Graph/DAG) 설계 및 제안
2. Do-calculus 또는 Propensity Score Matching을 활용한 인과 효과 추정
3. 교란 변수(Confounding Factors) 통제 방법 설명
4. "만약 ~했다면(Counterfactual)" 시나리오 분석 결과 해석
```

## 🎯 7. 인사이트 도출

### 7-1. 인사이트 발견
```
30년 경력자의 눈으로 이 분석 결과에서 놓치기 쉬운 중요한 인사이트를 찾아줘.
1. 명확한 인사이트 3가지
2. 각 인사이트의 비즈니스 임팩트
3. 추가 분석이 필요한 부분
4. 액션 아이템 제시
```

### 7-2. 리포트 작성
```
이 분석 결과를 비즈니스 리포트로 작성해줘.
대상: [경영진/실무진/일반직원]
포함 내용:
- Executive Summary
- 주요 발견사항
- 데이터 기반 제언
- 시각자료 구성 가이드
```

## 🔧 8. 문제 해결

### 8-1. 에러 해결
```
다음 에러가 발생했어: [에러 메시지]
1. 에러의 원인
2. 해결 방법 (여러 가지)
3. 향후 예방 방법
4. 관련 학습 자료
```

### 8-2. 코드 리뷰
```
내가 작성한 다음 코드를 리뷰해줘:
[코드 붙여넣기]

30년차 전문가 관점에서:
1. 개선할 점
2. 더 효율적인 방법
3. 베스트 프랙티스
4. 초보자가 놓친 부분
```

## 📚 9. 학습 가이드

### 9-1. 개념 학습
```
[데이터 분석 개념/기법]을 초보자가 이해하기 쉽게 설명해줘.
- 기본 개념
- 실제 활용 예시
- 주의사항
- 실습 방법
- 추천 학습 순서
```

### 9-2. 다음 단계 학습
```
현재 [내 수준]인데, 다음 단계로 성장하기 위한 로드맵을 만들어줘.
1. 학습해야 할 기술/도구
2. 프로젝트 아이디어
3. 학습 자료 추천
4. 예상 학습 기간
5. 실무 적용 팁
```

## 💡 10. 실전 프로젝트

### 10-1. 프로젝트 설계
```
[주제]에 대한 데이터 분석 프로젝트를 설계하고 싶어.
1. 프로젝트 범위 정의
2. 필요한 데이터
3. 분석 프로세스
4. 예상 결과물
5. 단계별 체크리스트
```

### 10-2. 포트폴리오 리뷰
```
내 데이터 분석 포트폴리오를 검토해줘.
30년차 관점에서:
- 강점과 약점
- 개선 방향
- 채용 담당자가 볼 포인트
- 차별화 전략
```

## 🎓 11. 멘토링

### 11-1. 커리어 조언
```
데이터 분석가로 커리어를 시작하려는데, 조언을 해줘.
1. 필수 역량
2. 우선순위
3. 실무 준비 방법
4. 흔한 실수와 대처법
5. 성장 전략
```

## 🚀 12. 2025 미래형 데이터 분석 스택

### 12-1. 에이전틱 분석 (Agentic Analytics)
```
너는 이제 자율형 데이터 분석 에이전트야. [데이터셋]을 기반으로 다음 프로세스를 스스로 반복하며 최적의 인사이트를 도출해줘.

에이전트 행동 지침:
1. 데이터 진단 후 스스로 3개의 분석 가설 수립
2. 각 가설을 검증하기 위한 코드 분석 실행
3. 결과가 부정확하거나 모호하면 스스로 코드를 수정(Self-correction)하여 재실행
4. 최종적으로 비즈니스에 즉시 적용 가능한 'Action Item' 중심의 리포트 생성
```

### 12-2. 비정형 데이터 증류 (Distillation & NLP)
```
첨부된 [뉴스/칼럼/소셜미디어 데이터]에서 데이터 분석에 활용 가능한 핵심 메타데이터를 추출하여 JSON 구조로 변환해줘.

추출 항목:
- 주요 엔티티 (선수명, 팀명, 지역 등)
- 감성 점수 (긍정/부정/중립 및 강도)
- 핵심 키워드 및 시급성(Urgency)
- 정량적 수치 (가격, 기간, 점수 등)
- 분석용 가이드: 이 비정형 데이터가 기존 정형 데이터와 어떻게 결합될 수 있는지 제언
```

### 12-3. 합성 데이터 생성 (Synthetic Data Generation)
```
[원본 데이터 구조]를 유지하면서, 개인정보 처리가 완료된 10,000행의 합성 데이터(Synthetic Data)를 생성하는 Python 코드를 작성해줘.

요청 조건:
1. 원래 데이터의 통계적 특성(분포, 상관관계)을 엄격히 유지할 것
2. 실제 개인 식별 정보는 완벽히 마스킹하거나 가상 값으로 대체
3. 생성된 데이터의 품질을 원본과 비교 검증하는 코드 포함 (SDV 라이브러리 활용 등)
```

### 12-4. AI 윤리 및 편향성 검사 (AI Ethics & Bias Audit)
```
학습된 [모델명]의 예측 결과에 대해 성별, 연령, 지역 등에 따른 편향성(Bias)이 존재하는지 정밀 진단해줘.

체크리스트:
1. 그룹별 성능 평가지표(Precision/Recall) 차이 분석
2. 공정성 지표(Demographic Parity, Equalized Odds) 계산
3. 편향이 발견될 경우 이를 완화(Mitigation)하기 위한 데이터 재샘플링 또는 알고리즘 수정 제안
```

### 12-5. 데이터 분석 결과의 로컬 서비스화 (Local Appification)
```
[중요: 보안 및 로컬 실행 원칙] 
숨고(Soomgo) 고객 데이터와 같은 민감한 정보를 포함하고 있으므로, 모든 분석 결과물은 외부 서버가 아닌 **반드시 로컬 환경(Local Machine)**에서만 실행되는 앱이나 API로 구성해줘.

포함 내용:
1. 보안 우선(Privacy-First): 데이터가 외부 네트워크로 유출되지 않도록 하는 로컬 실행 구조 설계
2. Streamlit 활용: 로컬 대시보드 구성을 위한 인터랙티브 위젯 및 실시간 데이터 시각화
3. FastAPI 활용: 로컬 네트워크 내에서 타 서비스와 연동할 수 있는 경량 API 설계
4. 성능 최적화: 로컬 자원(CPU/RAM)을 효율적으로 쓰기 위한 캐싱(st.cache_data) 및 경량 모델 로딩
5. 보안 가이드: `.env` 파일을 통한 환경 변수 관리 및 로컬 실행 로그 보안 처리
```

## 🧠 13. [고급] 논리적 추론 및 정밀 코딩 스택 (Claude-Level Logic)

### 13-1. 사고의 사슬 강제 (Chain-of-Thought Implementation)
```
결론부터 내지 말고, 다음 4단계에 따라 사고의 과정을 먼저 텍스트로 서술한 뒤 코드를 작성해.

요청 단계:
1. [상황 분석]: 데이터의 특성과 도메인 제약 사항 파악
2. [로직 설계]: 각 단계별 알고리즘 및 데이터 흐름 설계
3. [의사코드]: 실제 코딩 전 논리적 뼈대(Pseudocode) 작성
4. [최종 구현]: 주석이 포함된 완성된 코드 출력
```

### 13-2. 자가 비판 및 최적화 (Self-Reflection Loop)
```
코드를 생성한 후, 너 스스로 다음 3가지 질문에 답변하고 코드를 1회 수정(Refine)하여 최종안을 제시해.

자기 성찰 체크리스트:
1. "이 코드에 논리적 오류나 무한 루프 가능성이 있는가?"
2. "메모리와 CPU 자원을 더 아낄 수 있는 방법이 있는가?"
3. "숨고(Soomgo)와 같은 민감한 고객 데이터 처리 시 보안 결함이 없는가?"
```

### 13-3. 엣지 케이스 대응 (Robust Coding)
```
정상 데이터뿐만 아니라, 예상치 못한 상황에서도 앱이 죽지 않도록 예외 처리가 강화된 코드를 작성해.

필수 포함 사항:
1. 데이터 결측치(NaN/Null) 발생 시 대응 로직
2. 데이터 타입 불일치 시 자동 변환 및 에러 핸들링
3. 로컬 환경 리소스 부족 시 처리 방식
4. 사용자 입력 오류에 대한 친절한 안내 메시지 도출
```

## 🌐 14. 자동화된 AI 연구 및 지식 업데이트 (Continuous Learning)

### 14-1. 전세계 최신 논문 및 기술 소스 수집
```
전세계 최신 AI 논문(arXiv 등), 엔지니어링 블로그, 그리고 지정된 뉴스 소스(PyTorch KR, AITimes, Geeknews, JoshNews)를 긁어와서 핵심만 정리해줘.

요청 사항:
1. 요약 보고서: 오늘 수집된 정보 중 데이터 분석가에게 가장 중요한 3가지 뉴스/기술 선정 및 분석
2. 지식 증류: 수집된 내용을 제미나이(Antigravity)가 학습하기 좋은 형태의 'Knowledge Block'으로 변환
3. 시사점: 이 기술이 실제 K-리그 데이터나 숨고 데이터 분석에 어떻게 적용될 수 있는지 제언
```

### 14-2. 링크드인(LinkedIn) 및 커뮤니티 데이터 연동
```
내가 터미널로 넘겨주는 링크드인 복사 내용이나 커뮤니티 포스팅을 분석해서 지식 베이스에 추가해줘.

분석 포인트:
- 업계 리더들의 최신 인사이트 및 비즈니스 트렌드 추출
- 실무에서 쓰이는 최신 라이브러리/도구 언급 여부 파악
- 나중에 참고할 수 있도록 태그(#Insight, #TechTrend)를 달아 로컬 메모리에 저장
```

### 14-3. 데일리 자동화 워크플로우 및 스토리지 최적화
```
환경 설정 및 자원 관리:
- 매일 아침 인터넷 연결 시 스크립트 실행 (scripts/daily_research_automation.sh)
- 프로세스: 수집 -> 분석 -> 제미나이 업데이트(SKILL.md 반영) -> [중요] 원본 삭제
- 저장 공간 최적화: 학습이 완료되어 SKILL.md에 인사이트가 저장된 것이 확인되면, 수백 MB에 달할 수 있는 중간 스크래핑 데이터 및 임시 파일들을 즉시 삭제하여 맥의 성능 유지.
```

---

## 📌 활용 팁

### 프롬프트 사용 시 체크리스트
- [ ] 구체적인 상황/데이터 설명
- [ ] 원하는 결과물 명시
- [ ] 내 현재 수준 언급
- [ ] 추가 질문 준비

### 더 좋은 답변을 받는 방법
1. **구체적으로**: "분석해줘" → "고객 세그먼트별 구매 패턴을 분석해줘"
2. **맥락 제공**: 데이터의 출처, 목적, 제약사항 공유
3. **단계별 진행**: 복잡한 분석은 여러 단계로 나눠서 질문
4. **피드백**: 원하는 방향이 아니면 추가 설명 요청

### 데이터 첨부 시 팁
- CSV, Excel 파일 직접 업로드 가능
- 큰 파일은 샘플 데이터로 먼저 테스트
- 데이터 설명(컬럼 의미, 수집 방법 등) 함께 제공

---

**저장 방법**: 이 문서를 복사해서 메모장, Notion, 또는 원하는 곳에 저장하세요!
필요할 때마다 해당하는 프롬프트를 복사해서 Claude에게 질문하면 됩니다.